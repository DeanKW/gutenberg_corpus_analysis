{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f715e7a-181d-4a8f-8150-6bf449cf0c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from misc_utils import dataset_filtering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import textstat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, make_scorer)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aefedac-cbb1-4b9b-a3c6-a2a2546385d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo_path = '/Users/hunterworssam/Datascience'\n",
    "gutenberg_repo_path = os.path.join(git_repo_path, 'gutenberg')\n",
    "gutenberg_analysis_repo = os.path.join(git_repo_path, 'gutenberg_corpus_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0565518-f89d-4505-887e-54c4f753e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "train = pd.read_csv('final_train.csv')\n",
    "test = pd.read_csv('final_test.csv')\n",
    "val = pd.read_csv('final_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d79c18e3-a6dd-4ea8-b154-cc0edf7a7a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa80022b-8e3e-47e0-a6b9-d3f8de15e0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c760961-84bb-40eb-b1bc-6de298b2fa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b56f1e70-db3e-4d16-ba4d-7dc1420a7618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2439</td>\n",
       "      <td>PG12810</td>\n",
       "      <td>Uncle Sam's Boys with Pershing's Troops: Or, D...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>78</td>\n",
       "      <td>{'World War, 1914-1918 -- Juvenile fiction', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2446</td>\n",
       "      <td>PG12819</td>\n",
       "      <td>Dick Prescott's Second Year at West Point: Or,...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>94</td>\n",
       "      <td>{'United States Military Academy -- Juvenile f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25920</td>\n",
       "      <td>PG40605</td>\n",
       "      <td>The Motor Boat Club at Nantucket; or, The Myst...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>189</td>\n",
       "      <td>{'Motorboats -- Juvenile fiction', 'Nantucket ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55435</td>\n",
       "      <td>PG8153</td>\n",
       "      <td>The Young Engineers in Arizona; or, Laying Tra...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>190</td>\n",
       "      <td>{'Civil engineers -- Fiction', 'Arizona -- Fic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32899</td>\n",
       "      <td>PG48863</td>\n",
       "      <td>The Motor Boat Club off Long Island; or, A Dar...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>85</td>\n",
       "      <td>{'Motorboats -- Juvenile fiction', 'Long Islan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id                                              title  \\\n",
       "0        2439  PG12810  Uncle Sam's Boys with Pershing's Troops: Or, D...   \n",
       "1        2446  PG12819  Dick Prescott's Second Year at West Point: Or,...   \n",
       "2       25920  PG40605  The Motor Boat Club at Nantucket; or, The Myst...   \n",
       "3       55435   PG8153  The Young Engineers in Arizona; or, Laying Tra...   \n",
       "4       32899  PG48863  The Motor Boat Club off Long Island; or, A Dar...   \n",
       "\n",
       "                               author  authoryearofbirth  authoryearofdeath  \\\n",
       "0  Hancock, H. Irving (Harrie Irving)             1868.0             1922.0   \n",
       "1  Hancock, H. Irving (Harrie Irving)             1868.0             1922.0   \n",
       "2  Hancock, H. Irving (Harrie Irving)             1868.0             1922.0   \n",
       "3  Hancock, H. Irving (Harrie Irving)             1868.0             1922.0   \n",
       "4  Hancock, H. Irving (Harrie Irving)             1868.0             1922.0   \n",
       "\n",
       "  language  downloads                                           subjects  \n",
       "0   ['en']         78  {'World War, 1914-1918 -- Juvenile fiction', '...  \n",
       "1   ['en']         94  {'United States Military Academy -- Juvenile f...  \n",
       "2   ['en']        189  {'Motorboats -- Juvenile fiction', 'Nantucket ...  \n",
       "3   ['en']        190  {'Civil engineers -- Fiction', 'Arizona -- Fic...  \n",
       "4   ['en']         85  {'Motorboats -- Juvenile fiction', 'Long Islan...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f997b60b-1f25-496b-a912-b362181c77f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>712</td>\n",
       "      <td>PG108</td>\n",
       "      <td>The Return of Sherlock Holmes</td>\n",
       "      <td>Doyle, Arthur Conan</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>1930.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>3348</td>\n",
       "      <td>{'Detective and mystery stories, English', 'Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>821</td>\n",
       "      <td>PG1101</td>\n",
       "      <td>The Second Part of King Henry the Sixth</td>\n",
       "      <td>Shakespeare, William</td>\n",
       "      <td>1564.0</td>\n",
       "      <td>1616.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>131</td>\n",
       "      <td>{'Historical drama', 'Great Britain -- History...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>1404</td>\n",
       "      <td>PG11641</td>\n",
       "      <td>Over There: War Scenes on the Western Front</td>\n",
       "      <td>Bennett, Arnold</td>\n",
       "      <td>1867.0</td>\n",
       "      <td>1931.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>101</td>\n",
       "      <td>{'Bennett, Arnold, 1867-1931', 'World War, 191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>1525</td>\n",
       "      <td>PG11803</td>\n",
       "      <td>U.S. Copyright Renewals, 1951 January - June</td>\n",
       "      <td>Library of Congress. Copyright Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['en']</td>\n",
       "      <td>179</td>\n",
       "      <td>{'Copyright -- United States -- Catalogs'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1572</td>\n",
       "      <td>PG11846</td>\n",
       "      <td>U.S. Copyright Renewals, 1972 July - December</td>\n",
       "      <td>Library of Congress. Copyright Office</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['en']</td>\n",
       "      <td>158</td>\n",
       "      <td>{'Copyright -- United States -- Catalogs'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0       id  \\\n",
       "0             4         712    PG108   \n",
       "1             6         821   PG1101   \n",
       "2            12        1404  PG11641   \n",
       "3            14        1525  PG11803   \n",
       "4            16        1572  PG11846   \n",
       "\n",
       "                                           title  \\\n",
       "0                  The Return of Sherlock Holmes   \n",
       "1        The Second Part of King Henry the Sixth   \n",
       "2    Over There: War Scenes on the Western Front   \n",
       "3   U.S. Copyright Renewals, 1951 January - June   \n",
       "4  U.S. Copyright Renewals, 1972 July - December   \n",
       "\n",
       "                                  author  authoryearofbirth  \\\n",
       "0                    Doyle, Arthur Conan             1859.0   \n",
       "1                   Shakespeare, William             1564.0   \n",
       "2                        Bennett, Arnold             1867.0   \n",
       "3  Library of Congress. Copyright Office                NaN   \n",
       "4  Library of Congress. Copyright Office                NaN   \n",
       "\n",
       "   authoryearofdeath language  downloads  \\\n",
       "0             1930.0   ['en']       3348   \n",
       "1             1616.0   ['en']        131   \n",
       "2             1931.0   ['en']        101   \n",
       "3                NaN   ['en']        179   \n",
       "4                NaN   ['en']        158   \n",
       "\n",
       "                                            subjects  \n",
       "0  {'Detective and mystery stories, English', 'Ho...  \n",
       "1  {'Historical drama', 'Great Britain -- History...  \n",
       "2  {'Bennett, Arnold, 1867-1931', 'World War, 191...  \n",
       "3         {'Copyright -- United States -- Catalogs'}  \n",
       "4         {'Copyright -- United States -- Catalogs'}  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd61a8f1-e19d-4789-bd6a-1107654e12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the word, line and token counts\n",
    "def enrich_dataframe(df):\n",
    "    count_path = os.path.join(gutenberg_repo_path, 'data', 'counts')\n",
    "    text_path = os.path.join(gutenberg_repo_path, 'data', 'text')\n",
    "    token_path = os.path.join(gutenberg_repo_path, 'data', 'tokens')\n",
    "\n",
    "    df['word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_word_count(pid, count_path))\n",
    "    df['unique_word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_unique_word_count(pid, count_path))\n",
    "    df['line_count'] = df['id'].apply(lambda pid: dataset_filtering.get_line_count(pid, text_path))\n",
    "    df['token_count'] = df['id'].apply(lambda pid: dataset_filtering.get_token_count(pid, token_path))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae658142-8f2a-47a8-8842-f27cef624328",
   "metadata": {},
   "source": [
    "Token count, unique word count and line count were not added because the nltk module was not downloaded properly at the time of the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65c93e63-5dd6-461d-ac80-02f349ae9979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1920, 13)\n",
      "Val shape: (240, 14)\n",
      "Test shape: (240, 14)\n"
     ]
    }
   ],
   "source": [
    "# Apply to all datasets\n",
    "train = enrich_dataframe(train)\n",
    "val = enrich_dataframe(val)\n",
    "test = enrich_dataframe(test)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Val shape:\", val.shape)\n",
    "print(\"Test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1917f343-327c-4c9e-b0aa-a6bfa15688f2",
   "metadata": {},
   "source": [
    "Add text and raw data from gutenberg import to the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6adc958-0ad4-4267-b220-b552c492262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_folder = os.path.join(gutenberg_repo_path, 'data', 'text')\n",
    "\n",
    "# Function to load text for a given Project Gutenberg ID\n",
    "def load_book_text(pg_id):\n",
    "    filename = f'{pg_id}_text.txt'\n",
    "    filepath = os.path.join(text_folder, filename)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        return None  # or \"\", depending on preference\n",
    "\n",
    "# Apply the function to each row\n",
    "train['text'] = train['id'].apply(load_book_text)\n",
    "val['text'] = val['id'].apply(load_book_text)\n",
    "test['text'] = test['id'].apply(load_book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c3a5ddb-31e3-4a39-95a1-1de9806217a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_folder = os.path.join(gutenberg_repo_path, 'data', 'raw')\n",
    "\n",
    "# Function to load text for a given Project Gutenberg ID\n",
    "def load_book_text_raw(pg_id):\n",
    "    filename = f'{pg_id}_raw.txt'\n",
    "    filepath = os.path.join(raw_folder, filename)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        return None  # or \"\", depending on preference\n",
    "\n",
    "# Apply the function to each row\n",
    "train['raw'] = train['id'].apply(load_book_text_raw)\n",
    "val['raw'] = val['id'].apply(load_book_text_raw)\n",
    "test['raw'] = test['id'].apply(load_book_text_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f15d22-e5d8-4e9d-837f-ca67351e3e68",
   "metadata": {},
   "source": [
    "Confirm all rows have text data appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e81d07da-f151-46eb-802f-a0d5233953ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing raw files: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing raw files:\", train['raw'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a16174de-b566-42c3-87ce-808848c9b3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing text files: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing text files:\", train['text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e04246a4-3cf7-4d55-8dfc-f8139cd02738",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_text_rows = train[train['text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fe3606f-d279-442b-bc45-df68c18c5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_first_50_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text  # Leave NaN or None untouched\n",
    "    words = text.split()\n",
    "    return ' '.join(words[50:])  # Remove first 50 words\n",
    "\n",
    "# Preprocess text column\n",
    "train['text'] = train['text'].apply(remove_first_50_words)\n",
    "val['text'] = val['text'].apply(remove_first_50_words)\n",
    "test['text'] = test['text'].apply(remove_first_50_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c907ff-762e-461b-90fb-ae76893fbafd",
   "metadata": {},
   "source": [
    "#### Texts have been loaded in, now we can begin TF-IDF Vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb9ffc0f-1393-4b54-93c2-89c07640ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english', # Removes a lot of common english words like it, and, that, is etc. Uses predifined scikit list of common english words.\n",
    "    sublinear_tf=True, # Uses logarithmic word frequency weighting, reducing the weight of extremely frequent terms & helps prevent domination by larger text files\n",
    "    max_features=15000, # Consideration for both overfitting and computational requirements.\n",
    "    ngram_range=(1,3)\n",
    ")\n",
    "# Try iterating through 1000, 5000, 10000, 20000 max features (Could reduce overfitting (1, 1)\t\n",
    "# 1,000 – 10,000 (1, 2)\t5,000 – 20,000 (2, 3)\t5,000 – 15,000 (use SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f18d575-9c5e-44f8-93dc-6c734ffebe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train['text'])\n",
    "X_val = vectorizer.transform(val['text'])\n",
    "X_test = vectorizer.transform(test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ef155-c63a-49e5-9318-4cbe9e847fed",
   "metadata": {},
   "source": [
    "Create seperate dataframe for TF-IDF Vectorization output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cec915b3-0ab7-4ef7-9329-bf2018df6f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920, 15000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f5cb8d6-c76c-4bb5-8677-06ec1f2f9d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '000 000' '10' '10 min' '10 min sd' '100' '101' '102' '103']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd0ad70-1244-4ef6-8643-0a124526aba9",
   "metadata": {},
   "source": [
    "Add stylometric features to the TF-IDF feature array. Twelve features are added here, all related to punctuation frequency and sentence/word length averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9896f8ef-79a5-4991-8fe0-9a900ff388cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract stylometric features from a single text\n",
    "def extract_stylometric_features(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return [0] * 12  # updated to match total number of features\n",
    "\n",
    "    # Use NLTK tokenizers with explicit language call\n",
    "    words = word_tokenize(text, language='english', preserve_line=True)\n",
    "    sentences = sent_tokenize(text, language='english')\n",
    "    chars = list(text)\n",
    "\n",
    "    word_lengths = [len(w) for w in words if w.isalpha()]\n",
    "    total_words = len(words)\n",
    "    unique_words = len(set(w.lower() for w in words if w.isalpha()))\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    avg_word_length = np.mean(word_lengths) if word_lengths else 0\n",
    "    avg_sentence_length = total_words / total_sentences if total_sentences else 0\n",
    "    type_token_ratio = unique_words / total_words if total_words else 0\n",
    "\n",
    "    punctuation_counts = Counter(c for c in text if c in \".,!?;:-\")\n",
    "    punctuation_freqs = [punctuation_counts[p] / len(text) for p in \".,!?;:-\"]\n",
    "\n",
    "    uppercase_chars = sum(1 for c in text if c.isupper())\n",
    "    uppercase_ratio = uppercase_chars / len(text)\n",
    "\n",
    "    digit_ratio = sum(1 for c in text if c.isdigit()) / len(text)\n",
    "\n",
    "    return [\n",
    "        avg_word_length,\n",
    "        avg_sentence_length,\n",
    "        type_token_ratio,\n",
    "        *punctuation_freqs,\n",
    "        uppercase_ratio,\n",
    "        digit_ratio\n",
    "    ]\n",
    "\n",
    "# List of feature names\n",
    "stylo_feature_names = [\n",
    "    \"avg_word_length\",\n",
    "    \"avg_sentence_length\",\n",
    "    \"type_token_ratio\",\n",
    "    \"period_freq\", \"comma_freq\", \"exclam_freq\",\n",
    "    \"question_freq\", \"semicolon_freq\", \"colon_freq\", \"dash_freq\",\n",
    "    \"uppercase_ratio\", \"digit_ratio\"\n",
    "]\n",
    "\n",
    "# Apply to your dataset\n",
    "def get_stylometric_features(df):\n",
    "    features = df['text'].apply(extract_stylometric_features)\n",
    "    return pd.DataFrame(features.tolist(), columns=stylo_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f468ef0-d8f4-4673-8a72-75b98ef5227d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   avg_word_length  avg_sentence_length  type_token_ratio  period_freq  \\\n",
      "0         4.250333            15.664977          0.078897     0.010975   \n",
      "1         4.248404            14.571429          0.081391     0.013303   \n",
      "2         4.066894            20.021214          0.072704     0.012683   \n",
      "3         4.139240            20.450896          0.075695     0.012977   \n",
      "4         4.125742            21.993048          0.069692     0.012717   \n",
      "\n",
      "   comma_freq  exclam_freq  question_freq  semicolon_freq  colon_freq  \\\n",
      "0    0.012713     0.001262       0.001098        0.000138    0.000208   \n",
      "1    0.014442     0.001212       0.001377        0.000256    0.000176   \n",
      "2    0.013631     0.001044       0.001168        0.000244    0.000173   \n",
      "3    0.013200     0.001576       0.001345        0.000166    0.000083   \n",
      "4    0.015411     0.000932       0.001211        0.000102    0.000185   \n",
      "\n",
      "   dash_freq  uppercase_ratio  digit_ratio  \n",
      "0   0.002203         0.027425     0.000093  \n",
      "1   0.002391         0.030510     0.000172  \n",
      "2   0.001009         0.035503     0.000184  \n",
      "3   0.002146         0.028415     0.000000  \n",
      "4   0.001218         0.031257     0.000656  \n"
     ]
    }
   ],
   "source": [
    "# For train set\n",
    "train_stylo = get_stylometric_features(train)\n",
    "\n",
    "# For val and test\n",
    "val_stylo = get_stylometric_features(val)\n",
    "test_stylo = get_stylometric_features(test)\n",
    "\n",
    "# Preview\n",
    "print(train_stylo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cda95a-e08a-4f91-ad1d-5bf38e3c0da5",
   "metadata": {},
   "source": [
    "Add common readability quantification metrics to the dataset. Our training set will now be 1920 x (TF-IDF value + 12 + 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1187045-d093-4fa8-b594-ae7932622b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_metrics(text):\n",
    "    return [\n",
    "        textstat.flesch_reading_ease(text),\n",
    "        textstat.flesch_kincaid_grade(text),\n",
    "        textstat.gunning_fog(text),\n",
    "        textstat.smog_index(text),\n",
    "        textstat.coleman_liau_index(text),\n",
    "        textstat.automated_readability_index(text)\n",
    "    ]\n",
    "\n",
    "readability_feature_names = [\n",
    "    'flesch_reading_ease',\n",
    "    'flesch_kincaid_grade',\n",
    "    'gunning_fog',\n",
    "    'smog_index',\n",
    "    'coleman_liau_index',\n",
    "    'automated_readability_index'\n",
    "]\n",
    "\n",
    "# Apply to a DataFrame\n",
    "def get_readability_features(df):\n",
    "    scores = df['text'].apply(readability_metrics)\n",
    "    return pd.DataFrame(scores.tolist(), columns=readability_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73e6529b-0924-4695-ae75-5928ed8e29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get readability feature matrices\n",
    "train_readability = get_readability_features(train)\n",
    "val_readability = get_readability_features(val)\n",
    "test_readability = get_readability_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe98090a-6101-4cc6-ab6b-4da3c865eee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.78</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.68</td>\n",
       "      <td>9.1</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75.40</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.20</td>\n",
       "      <td>8.6</td>\n",
       "      <td>7.29</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74.90</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.16</td>\n",
       "      <td>8.4</td>\n",
       "      <td>6.77</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75.40</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.59</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.24</td>\n",
       "      <td>8.3</td>\n",
       "      <td>7.30</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  flesch_kincaid_grade  gunning_fog  smog_index  \\\n",
       "0                73.78                   6.5         6.68         9.1   \n",
       "1                75.40                   5.9         6.20         8.6   \n",
       "2                74.90                   6.1         6.16         8.4   \n",
       "3                75.40                   5.9         6.00         8.6   \n",
       "4                74.59                   6.2         6.24         8.3   \n",
       "\n",
       "   coleman_liau_index  automated_readability_index  \n",
       "0                7.71                          7.7  \n",
       "1                7.29                          6.9  \n",
       "2                6.77                          6.7  \n",
       "3                6.83                          6.6  \n",
       "4                7.30                          7.1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_readability.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c34bc545-46bb-46e1-9175-29edff7045bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontally concatenate\n",
    "train_extra = pd.concat([train_stylo, train_readability], axis=1)\n",
    "val_extra = pd.concat([val_stylo, val_readability], axis=1)\n",
    "test_extra = pd.concat([test_stylo, test_readability], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110009a-ab8d-48c0-8f5a-edbc6afb8652",
   "metadata": {},
   "source": [
    "### Features with larger numeric ranges dominate the distance calculation within kNN, therefore we must scale our 18 additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d346bdc7-f0a8-42dd-a2d5-2eae25f0c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale only the stylometric + readability features\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_extra)  # shape: (1920, 18)\n",
    "val_scaled = scaler.transform(val_extra)\n",
    "test_scaled = scaler.transform(test_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a6c3d-24fd-46fe-8c6f-78a8121e19c0",
   "metadata": {},
   "source": [
    "### Way more features than observations. Let's implement truncated SVD, which is a principal component analysis for spare matrices. We only want to implement this on the TF-IDF features since they are sparse. Note that this step was omitted for both NB and RF modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3004a25f-db81-4f46-b3ef-957377675550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate here through various numbers of components (100 - 1000)\n",
    "# Reduce TF-IDF part only\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "X_train_reduced = svd.fit_transform(X_train)\n",
    "X_val_reduced   = svd.transform(X_val)\n",
    "X_test_reduced  = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45601092-964d-4b73-9e3b-248c9956f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduces dimensionality in a continuous, information-preserving way. \n",
    "# Projects your 10,018-dimensional space into (say) 300 components based on variance and structure in the data. \n",
    "# Handles sparsity and correlation automatically, Works great for distance-based or margin-based models like: \n",
    "# k-Nearest Neighbors (kNN), Logistic Regression, SVM, Random Forest (can still benefit from lower noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39299828-0019-4d2e-ba62-54a1c4d3f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert extra features to sparse format\n",
    "train_extra_sparse = csr_matrix(train_scaled)\n",
    "val_extra_sparse = csr_matrix(val_scaled)\n",
    "test_extra_sparse = csr_matrix(test_scaled)\n",
    "\n",
    "X_train_reduced = csr_matrix(X_train_reduced)\n",
    "X_val_reduced   = csr_matrix(X_val_reduced)\n",
    "X_test_reduced  = csr_matrix(X_test_reduced)\n",
    "\n",
    "# Stack with TF-IDF matrices\n",
    "X_train_combined = hstack([X_train_reduced, train_extra_sparse])\n",
    "X_val_combined   = hstack([X_val_reduced, val_extra_sparse])\n",
    "X_test_combined  = hstack([X_test_reduced, test_extra_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8203e082-d4f1-41ea-9f1e-6fc6cbf42bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920, 318)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6143e96c-4f26-4f04-9923-9bb6791489cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels from original dataset\n",
    "y_train = train['author']\n",
    "y_val = val['author']\n",
    "y_test = test['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb95cc1f-f06b-4888-be1a-05672e5af3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    # Train and predict\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Metrics (set zero_division=0 to silence warnings)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "    precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Print performance\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5e8972d-614e-46f1-ab71-e38ff81c8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=5,        # or try others: 3, 7, 10\n",
    "    metric='euclidean',   # or 'manhattan', 'cosine'\n",
    "    weights='uniform'     # or 'distance'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df569a04-8eae-4528-bad1-4570dfac6d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: KNeighborsClassifier\n",
      "Accuracy:  0.9167\n",
      "F1 Score:  0.9100\n",
      "Precision: 0.9273\n",
      "Recall:    0.9167\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "knn_model, y_pred_knn = evaluate_model(\n",
    "    model=knn,\n",
    "    X_train=X_train_combined,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val_combined,\n",
    "    y_val=y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc711f4-d3e8-4095-b2c3-91ff2ec82f2c",
   "metadata": {},
   "source": [
    "#### Note the vast amount of tuning possibilities. TF-IDF settings, SVD settings, +/- Stylo features and kNN settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e22442d4-e4c8-4ef9-80ee-345ae4cd2ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Define Parameters -----\n",
    "svd_components = [100, 300, 500, 800]  # Try different SVD reductions\n",
    "k_values = [3, 5, 7, 10]\n",
    "metrics = ['euclidean', 'cosine']\n",
    "weights = ['uniform', 'distance']\n",
    "\n",
    "# ----- Create parameter combinations -----\n",
    "param_combos = list(product(svd_components, k_values, metrics, weights))\n",
    "\n",
    "results = []\n",
    "\n",
    "# ----- Loop over combinations -----\n",
    "for n_comp, k, dist_metric, wt in param_combos:\n",
    "    # Step 1: Apply SVD (on TF-IDF part only)\n",
    "    svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "    X_train_reduced = svd.fit_transform(X_train)\n",
    "    X_val_reduced   = svd.transform(X_val)\n",
    "    X_test_reduced  = svd.transform(X_test)\n",
    "\n",
    "    # Step 2: Build and train kNN\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=k,\n",
    "        metric=dist_metric,\n",
    "        weights=wt\n",
    "    )\n",
    "    knn.fit(X_train_reduced, y_train)\n",
    "    y_val_pred = knn.predict(X_val_reduced)\n",
    "\n",
    "    # Step 3: Evaluate\n",
    "    acc = accuracy_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred, average='weighted', zero_division=0)\n",
    "    precision = precision_score(y_val, y_val_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_val, y_val_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Step 4: Save results\n",
    "    results.append({\n",
    "        'svd_components': n_comp,\n",
    "        'k_neighbors': k,\n",
    "        'distance_metric': dist_metric,\n",
    "        'weights': wt,\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "# ----- Export to CSV -----\n",
    "df_knn_results = pd.DataFrame(results)\n",
    "df_knn_results = df_knn_results.sort_values(by='f1', ascending=False)\n",
    "df_knn_results.to_csv('knn_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642828a2-92bf-4217-8f9c-367a900104fb",
   "metadata": {},
   "source": [
    "#### Now we can implement the top performing hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56bd31d7-1edc-4166-b734-b6ee8ad13538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Train Set Metrics:\n",
      "Accuracy:  1.0000\n",
      "F1 Score:  1.0000\n",
      "Precision: 1.0000\n",
      "Recall:    1.0000\n",
      "----------------------------------------\n",
      "📊 Validation Set Metrics:\n",
      "Accuracy:  0.9500\n",
      "F1 Score:  0.9438\n",
      "Precision: 0.9479\n",
      "Recall:    0.9500\n",
      "----------------------------------------\n",
      "📊 Test Set Metrics:\n",
      "Accuracy:  0.9375\n",
      "F1 Score:  0.9344\n",
      "Precision: 0.9565\n",
      "Recall:    0.9375\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Apply Truncated SVD to TF-IDF matrices\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "X_train_reduced = svd.fit_transform(X_train)\n",
    "X_val_reduced   = svd.transform(X_val)\n",
    "X_test_reduced  = svd.transform(X_test)\n",
    "\n",
    "# 2. Instantiate kNN with optimal parameters\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=3,\n",
    "    metric='cosine',\n",
    "    weights='distance'\n",
    ")\n",
    "\n",
    "# 3. Fit model\n",
    "knn.fit(X_train_reduced, y_train)\n",
    "\n",
    "# 4. Predict for each split\n",
    "y_train_pred = knn.predict(X_train_reduced)\n",
    "y_val_pred   = knn.predict(X_val_reduced)\n",
    "y_test_pred  = knn.predict(X_test_reduced)\n",
    "\n",
    "# 5. Define evaluation function\n",
    "def print_metrics(name, y_true, y_pred):\n",
    "    print(f\"📊 {name} Set Metrics:\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# 6. Print metrics\n",
    "print_metrics(\"Train\", y_train, y_train_pred)\n",
    "print_metrics(\"Validation\", y_val, y_val_pred)\n",
    "print_metrics(\"Test\", y_test, y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gutenberg_env]",
   "language": "python",
   "name": "conda-env-gutenberg_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
