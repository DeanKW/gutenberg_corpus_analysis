{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a56599ba-8b80-4ae4-baed-c5f5ce7ad919",
   "metadata": {},
   "source": [
    "# Compares different chunking variations as well as different TF-IDF Vectorization parameters, using lemmatized text\n",
    "\n",
    "Loops through all combinations of:\n",
    "\n",
    "**Num Chunks**: 10000, 15000, 20000, 5000, 40000, 80000,\n",
    "\n",
    "**Chunk Size**: (1,1), (1,2), (1,3), (2, 2), (2, 3), (3, 3)]\n",
    "\n",
    "**TF-IDF Max Features**: 5000, 10000, 15000, 20000\n",
    "\n",
    "**TF-IDF N-Grams**: (1,1), (1,2), (1,3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a2f49a-fdd3-41a5-b9c9-a111ac1c126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm, neural_network\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, make_scorer)\n",
    "import multiprocessing as mp\n",
    "\n",
    "from src.data_loader import GutenbergDataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c924525d-c6a6-403f-8db8-18484651cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd602c87-cec2-40b9-8f87-9e2452677907",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_repo_path = os.path.join(repos_path, 'gutenberg')\n",
    "gutenberg_analysis_repo = os.path.join(repos_path, 'gutenberg-analysis')\n",
    "\n",
    "src_dir = os.path.join(gutenberg_analysis_repo,'src')\n",
    "sys.path.append(src_dir)\n",
    "from data_io import get_book\n",
    "\n",
    "\n",
    "gutenberg_src_dir = os.path.join(gutenberg_repo_path,'src')\n",
    "sys.path.append(gutenberg_src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90f60c0-09d0-4765-ba70-dd26034c0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fold=os.path.join(gutenberg_repo_path, 'data', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fd0659c-f138-4831-8d41-b8e368c72d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='nikita_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb82ad9-9e70-4aa1-808c-810e980caff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>PG12810</td>\n",
       "      <td>Uncle Sam's Boys with Pershing's Troops: Or, D...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>78</td>\n",
       "      <td>{'World War, 1914-1918 -- Juvenile fiction', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>PG12819</td>\n",
       "      <td>Dick Prescott's Second Year at West Point: Or,...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>94</td>\n",
       "      <td>{'United States Military Academy -- Juvenile f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25920</th>\n",
       "      <td>PG40605</td>\n",
       "      <td>The Motor Boat Club at Nantucket; or, The Myst...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>189</td>\n",
       "      <td>{'Motorboats -- Juvenile fiction', 'Nantucket ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55435</th>\n",
       "      <td>PG8153</td>\n",
       "      <td>The Young Engineers in Arizona; or, Laying Tra...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>190</td>\n",
       "      <td>{'Civil engineers -- Fiction', 'Arizona -- Fic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32899</th>\n",
       "      <td>PG48863</td>\n",
       "      <td>The Motor Boat Club off Long Island; or, A Dar...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>85</td>\n",
       "      <td>{'Motorboats -- Juvenile fiction', 'Long Islan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "2439   PG12810  Uncle Sam's Boys with Pershing's Troops: Or, D...   \n",
       "2446   PG12819  Dick Prescott's Second Year at West Point: Or,...   \n",
       "25920  PG40605  The Motor Boat Club at Nantucket; or, The Myst...   \n",
       "55435   PG8153  The Young Engineers in Arizona; or, Laying Tra...   \n",
       "32899  PG48863  The Motor Boat Club off Long Island; or, A Dar...   \n",
       "\n",
       "                                   author  authoryearofbirth  \\\n",
       "2439   Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "2446   Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "25920  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "55435  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "32899  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "\n",
       "       authoryearofdeath language  downloads  \\\n",
       "2439              1922.0   ['en']         78   \n",
       "2446              1922.0   ['en']         94   \n",
       "25920             1922.0   ['en']        189   \n",
       "55435             1922.0   ['en']        190   \n",
       "32899             1922.0   ['en']         85   \n",
       "\n",
       "                                                subjects  \n",
       "2439   {'World War, 1914-1918 -- Juvenile fiction', '...  \n",
       "2446   {'United States Military Academy -- Juvenile f...  \n",
       "25920  {'Motorboats -- Juvenile fiction', 'Nantucket ...  \n",
       "55435  {'Civil engineers -- Fiction', 'Arizona -- Fic...  \n",
       "32899  {'Motorboats -- Juvenile fiction', 'Long Islan...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdl = GutenbergDataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ae787-9efc-4065-9abf-68787b5035e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = gdl.train_df.copy()\n",
    "val_df = gdl.val_df.copy()\n",
    "test_df = gdl.test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39401de8-aaa8-4b5d-91f0-bcc28097f7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e95d8c4b-22ed-4321-9f6d-6f25c28e4ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_that_thang(tokenized_text):\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV    \n",
    "    \n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(tokenized_text):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_final = word_lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            final_words.append(word_final)\n",
    "    return str(final_words)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1687df21-1296-4dd5-8aae-62d0640b1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_chunk( num_chunks, chunk_size, x_col='text', tokenized_col='tokenized'):\n",
    "    print(f'Starting {num_chunks} chunks of size {chunk_size}')\n",
    "    print('making random chunks')\n",
    "    train_df['chunks'] = train_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=num_chunks, chunk_size = chunk_size, overlap=False))\n",
    "    test_df['chunks'] = test_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=num_chunks, chunk_size = chunk_size, overlap=False))\n",
    "    val_df['chunks'] = val_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=num_chunks, chunk_size = chunk_size, overlap=False))\n",
    "    print('finished making random chunks')\n",
    "    print('beginning tokenization')\n",
    "\n",
    "    tokenized = process_map(word_tokenize, train_df['chunks'], max_workers=11, chunksize=5)\n",
    "    train_df[tokenized_col] = tokenized\n",
    "    tokenized = process_map(word_tokenize, val_df['chunks'], max_workers=11, chunksize=5)\n",
    "    val_df[tokenized_col] = tokenized\n",
    "    tokenized = process_map(word_tokenize, test_df['chunks'], max_workers=11, chunksize=5)\n",
    "    test_df[tokenized_col] = tokenized\n",
    "    print('finished tokenization')\n",
    "\n",
    "    print('beginning lemmatization')\n",
    "\n",
    "    lemmatized = process_map(lemmatize_that_thang, train_df['tokenized'], max_workers=11, chunksize=5)\n",
    "    train_df['lemmatized'] = lemmatized\n",
    "\n",
    "    lemmatized = process_map(lemmatize_that_thang, val_df['tokenized'], max_workers=11, chunksize=5)\n",
    "    val_df['lemmatized'] = lemmatized\n",
    "\n",
    "    lemmatized = process_map(lemmatize_that_thang, test_df['tokenized'], max_workers=11, chunksize=5)\n",
    "    test_df['lemmatized'] = lemmatized\n",
    "\n",
    "    print('finished lemmatization')\n",
    "    param_grid = {\n",
    "    'tfidf__max_features': [5000, 10000, 15000, 20000],#, 40000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1,3)]\n",
    "    }\n",
    "    iteration_times = []\n",
    "    # Create all combinations of parameters\n",
    "    param_combos = list(itertools.product(param_grid['tfidf__max_features'],\n",
    "                            param_grid['tfidf__ngram_range']))\n",
    "    outfold='/home/dean/Documents/gitRepos/gutenberg_corpus_analysis/SVM/chunk_results'\n",
    "    for max_feat, ngram_range in param_combos:\n",
    "        test_one_tfidf_combo(max_feat, ngram_range, outfold, f'numChunk{num_chunks}_chSize{chunk_size}')\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dab290ac-4ee4-4364-aeee-317508bd59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_that_thang(tokenized_text):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing PorterStemmer()\n",
    "    ps = PorterStemmer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word in tokenized_text:\n",
    "        if word not in stopwords.words('english'):\n",
    "            word_final = ps.stem(word)\n",
    "            final_words.append(word_final)\n",
    "\n",
    "    return str(final_words)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5fb5fba-2b33-40e7-9157-7c53e47a4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_tfidf_combo(max_feat, ngram_range, outfold, other_desc):\n",
    "    Tfidf_vect = TfidfVectorizer(\n",
    "    stop_words='english', # Removes a lot of common english words like it, and, that, is etc. Uses predifined scikit list of common english words.\n",
    "    sublinear_tf=True, # Uses logarithmic word frequency weighting, reducing the weight of extremely frequent terms & helps prevent domination by larger text files\n",
    "    max_features=max_feat, # Consideration for both overfitting and computational requirements.\n",
    "    ngram_range=ngram_range\n",
    "    )\n",
    "\n",
    "    x_col='chunks'\n",
    "    \n",
    "    print('performing tfidf vectorization')\n",
    "    Tfidf_vect.fit_transform(train_df[x_col])\n",
    "\n",
    "    Train_X, Train_Y = train_df[x_col], train_df['author']\n",
    "    Test_X, Test_Y = test_df[x_col], test_df['author']\n",
    "    Val_X, Val_Y = val_df[x_col], val_df['author']\n",
    "    \n",
    "    Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "    Val_X_Tfidf = Tfidf_vect.transform(Val_X)\n",
    "    preproc_desc = f'tfidf_feat{max_feat}_ngram{ngram_range[0]}-{ngram_range[1]}'\n",
    "    outfile=os.path.join(outfold, f'results_{other_desc}_{preproc_desc}.csv')\n",
    "    train_those_models(Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, preproc_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e30cd4-acc3-4cf5-b66c-21abadbb0dd7",
   "metadata": {},
   "source": [
    "# Lemmatization, English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0f61a-56cb-4aeb-8d7c-2b630cb8984e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c22f5467-8618-4b47-a5e1-c676878f9dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0dda7617-0a96-47f0-b648-4dfab9dded78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, model_description, preproc_desc, verbose=False):\n",
    "    # Train and predict\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Training and predicting took {end-start} seconds = {(end-start)/60} minutes')\n",
    "\n",
    "    results={}\n",
    "    for label, y_truth, y_pred in [('train', y_train, y_train_pred), \n",
    "                            ('validation', y_val, y_val_pred),\n",
    "                            ('test', y_test, y_test_pred)]:\n",
    "        # Metrics (set zero_division=0 to silence warnings)\n",
    "        acc = accuracy_score(y_truth, y_pred)\n",
    "        f1 = f1_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "        precision = precision_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        result_dict = {'accuracy': acc,\n",
    "                       'precision': precision,\n",
    "                       'recall' : recall,\n",
    "                       'f1' : f1}\n",
    "        results[label] = result_dict\n",
    "        \n",
    "        \n",
    "    # Print performance\n",
    "    if verbose:\n",
    "        print(f\"Model: {model.__class__.__name__}\")\n",
    "        print(f'Description: {model_description}')\n",
    "        print(f'Pre-processing: {preproc_desc}')\n",
    "        label_str=f'|{'':<15} ||  {'Accuracy':>15} | {'Precision':>15} | {'Recall':>15} | {'F1-Score':>15} |'\n",
    "        print(\"-\" * len(label_str))\n",
    "\n",
    "        print(label_str)\n",
    "        print(\"-\" * len(label_str))\n",
    "\n",
    "        for result_label, sub_res_dict in results.items():\n",
    "            output_str = f'|{result_label:<15} || '\n",
    "            \n",
    "            for key, val in sub_res_dict.items():\n",
    "                output_str += f' {val:15.4f} |'\n",
    "            print(output_str)\n",
    "    \n",
    "        print(\"-\" * len(label_str))\n",
    "\n",
    "    new_res_df = results_to_df(model.__class__.__name__, model_description, preproc_desc, results)\n",
    "    \n",
    "    # if os.path.exists(res_file):\n",
    "    #     old_res_df = pd.read_csv(res_file)\n",
    "    #     old_res_df.set_index(['model_type', 'description', 'preprocessing description', 'metric'], inplace=True)\n",
    "    \n",
    "    #     res_df = pd.concat([old_res_df, new_res_df])\n",
    "    #     res_df.to_csv(res_file)\n",
    "    # else:\n",
    "    #     new_res_df.to_csv(res_file)\n",
    "        \n",
    "\n",
    "    return new_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d6e45d8-7c1b-4e40-9678-174f11c5ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(model_type, model_desc, preproc_desc, result_dict):\n",
    "    res_df = pd.DataFrame.from_dict(result_dict)\n",
    "    res_df['model_type'] = model_type\n",
    "    res_df['description'] = model_desc\n",
    "    res_df['preprocessing description'] = preproc_desc\n",
    "    res_df.reset_index(inplace=True)\n",
    "    res_df.rename({'index':'metric'}, axis=1, inplace=True)\n",
    "    res_df.set_index(['model_type', 'description', 'preprocessing description', 'metric'], inplace=True)\n",
    "\n",
    "    return res_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b87d7175-fc47-4b69-985f-dfc0ca0afc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_those_models(Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, preprocessing_description):\n",
    "\n",
    "    models_to_eval = [\n",
    "        (svm.SVC(), 'default_settings'),\n",
    "        (svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto'), 'C=1, kernel Linear, deg 3, gamma auto'),\n",
    "        (svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='scale'), 'C=1, kernel rbf, deg 3 gamma scale'),\n",
    "        (svm.SVC(C=1.0, kernel='poly', degree=2, gamma='scale'), 'C=1, kernel poly, deg 2 gamma scale'),\n",
    "        (svm.SVC(C=1.0, kernel='poly', degree=3, gamma='scale'), 'C=1, kernel poly, deg 3 gamma scale'),\n",
    "        (svm.SVC(C=1.0, kernel='poly', degree=4, gamma='scale'), 'C=1, kernel poly, deg 4 gamma scale'),\n",
    "        (svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='scale'), 'C=1, kernel sigmoid, deg 3 gamma scale'),\n",
    "        (svm.LinearSVC(), 'default'),\n",
    "        #(svm.LinearSVC(C=10), 'C=10'),\n",
    "        #(svm.LinearSVC(C=10, max_iter=5000), 'C=10, max_it 5000'),\n",
    "        (svm.LinearSVC(C=10, max_iter=25000), 'C=10, max_it 25000'),\n",
    "        #(svm.LinearSVC(C=20, max_iter=5000), 'C=20, max_it 5000'),\n",
    "        (svm.LinearSVC(C=20, max_iter=25000), 'C=20, max_it 25000'),\n",
    "        (svm.LinearSVC(C=50, max_iter=25000), 'C=50, max_it 25000'),\n",
    "        #(svm.LinearSVC(C=10, multi_class='crammer_singer', max_iter=5000), 'C=10, crammer-singer, max_it 5000'),\n",
    "        (svm.LinearSVC(C=10, multi_class='crammer_singer', max_iter=25000), 'C=10, crammer-singer, max_it 25000'),\n",
    "       # (svm.LinearSVC(C=20, multi_class='crammer_singer', max_iter=5000), 'C=20, crammer-singer, max_it 5000'),\n",
    "        (svm.LinearSVC(C=20, multi_class='crammer_singer', max_iter=25000), 'C=20, crammer-singer, max_it 25000'),\n",
    "        (svm.NuSVC(), 'default'),\n",
    "        (svm.NuSVC(kernel='linear'), 'nu=0.5, kernel linear'),\n",
    "        (svm.NuSVC(nu=.75, kernel='linear'), 'nu=0.75, kernel linear'),\n",
    "        (svm.NuSVC(nu=.25, kernel='linear'), 'nu=0.25, kernel linear'),\n",
    "        (naive_bayes.MultinomialNB(), 'default'),\n",
    "        (neural_network.MLPClassifier(), 'default'),\n",
    "        (neural_network.MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000), 'hidden_layer_size 200, max it 1000'),\n",
    "        (neural_network.MLPClassifier(activation='logistic', max_iter=500), 'logistic act, 500 iter')\n",
    "        \n",
    "        \n",
    "    ]\n",
    "    param_list = []\n",
    "    for model, model_desc in models_to_eval:\n",
    "        params = (model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, model_desc, preprocessing_description)\n",
    "        param_list.append(params)\n",
    "\n",
    "    with mp.Pool(11) as p:\n",
    "        results = p.starmap(evaluate_model, param_list)\n",
    "\n",
    "   res_df = pd.concat(results)\n",
    "   res_df.to_csv(outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee8e64-48d5-4c3f-b407-39a1ff221caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_grid = {\n",
    "    'num_chunks':[1, 5, 10, 20, 50, 100],\n",
    "    'chunk_size':[100, 250, 500, 1000, 2000]\n",
    "}\n",
    "chunk_list = list(itertools.product(chunk_grid['num_chunks'],\n",
    "                            chunk_grid['chunk_size']))\n",
    "\n",
    "for num_chunks, chunk_size in chunk_list:\n",
    "    print(num_chunks, chunk_size)\n",
    "\n",
    "    test_one_chunk( num_chunks, chunk_size, x_col='text', tokenized_col='tokenized')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462af034-8085-4555-96a6-be409bd227a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9db37e-04d7-4e14-af73-9661e203e795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52ca9f-e29e-486a-b5f0-2dbe3668a862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2b1ae-d413-403f-84bf-15c28e93319e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e96014-b091-4f44-a208-fa81b4b04228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school-env",
   "language": "python",
   "name": "school-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
