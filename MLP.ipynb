{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32fbcf73-c7e7-4bf8-a071-b2ee482d6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import sys\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm, naive_bayes, neural_network\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, make_scorer)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96fa074-d4b2-4e83-ba36-0a5298ba51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "gutenberg_corpus_analysis_repo = os.path.join(repos_path, 'gutenberg_corpus_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d000760-41df-41e0-8b7e-88e35178d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_repo_path = os.path.join(repos_path, 'gutenberg')\n",
    "gutenberg_analysis_repo = os.path.join(repos_path, 'gutenberg-analysis')\n",
    "\n",
    "src_dir = os.path.join(gutenberg_analysis_repo,'src')\n",
    "sys.path.append(src_dir)\n",
    "from data_io import get_book\n",
    "\n",
    "\n",
    "gutenberg_src_dir = os.path.join(gutenberg_repo_path,'src')\n",
    "sys.path.append(gutenberg_src_dir)\n",
    "\n",
    "from metaquery import meta_query\n",
    "\n",
    "sys.path.append(gutenberg_corpus_analysis_repo)\n",
    "import misc_utils.dataset_filtering as dataset_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c3655ce-665a-4581-a96a-a7d629b0d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='nikita_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e80954b-c2c0-401d-8271-d0a96214f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = os.path.join(gutenberg_corpus_analysis_repo, dataset, 'final_train.csv')\n",
    "test_csv = os.path.join(gutenberg_corpus_analysis_repo, dataset, 'final_test.csv')\n",
    "val_csv = os.path.join(gutenberg_corpus_analysis_repo, dataset, 'final_val.csv')\n",
    "\n",
    "pg_catalog_filepath=os.path.join(gutenberg_repo_path, 'metadata', 'pg_catalog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe32a1-6966-478a-94d7-b8175e422c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1eb0c18-5463-45bd-b273-4436ae623067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>PG12810</td>\n",
       "      <td>Uncle Sam's Boys with Pershing's Troops: Or, D...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>78</td>\n",
       "      <td>{'World War, 1914-1918 -- Juvenile fiction', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>PG12819</td>\n",
       "      <td>Dick Prescott's Second Year at West Point: Or,...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>94</td>\n",
       "      <td>{'United States Military Academy -- Juvenile f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25920</th>\n",
       "      <td>PG40605</td>\n",
       "      <td>The Motor Boat Club at Nantucket; or, The Myst...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>189</td>\n",
       "      <td>{'Motorboats -- Juvenile fiction', 'Nantucket ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55435</th>\n",
       "      <td>PG8153</td>\n",
       "      <td>The Young Engineers in Arizona; or, Laying Tra...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>190</td>\n",
       "      <td>{'Civil engineers -- Fiction', 'Arizona -- Fic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32899</th>\n",
       "      <td>PG48863</td>\n",
       "      <td>The Motor Boat Club off Long Island; or, A Dar...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>85</td>\n",
       "      <td>{'Motorboats -- Juvenile fiction', 'Long Islan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "2439   PG12810  Uncle Sam's Boys with Pershing's Troops: Or, D...   \n",
       "2446   PG12819  Dick Prescott's Second Year at West Point: Or,...   \n",
       "25920  PG40605  The Motor Boat Club at Nantucket; or, The Myst...   \n",
       "55435   PG8153  The Young Engineers in Arizona; or, Laying Tra...   \n",
       "32899  PG48863  The Motor Boat Club off Long Island; or, A Dar...   \n",
       "\n",
       "                                   author  authoryearofbirth  \\\n",
       "2439   Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "2446   Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "25920  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "55435  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "32899  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "\n",
       "       authoryearofdeath language  downloads  \\\n",
       "2439              1922.0   ['en']         78   \n",
       "2446              1922.0   ['en']         94   \n",
       "25920             1922.0   ['en']        189   \n",
       "55435             1922.0   ['en']        190   \n",
       "32899             1922.0   ['en']         85   \n",
       "\n",
       "                                                subjects  \n",
       "2439   {'World War, 1914-1918 -- Juvenile fiction', '...  \n",
       "2446   {'United States Military Academy -- Juvenile f...  \n",
       "25920  {'Motorboats -- Juvenile fiction', 'Nantucket ...  \n",
       "55435  {'Civil engineers -- Fiction', 'Arizona -- Fic...  \n",
       "32899  {'Motorboats -- Juvenile fiction', 'Long Islan...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_csv, index_col='Unnamed: 0')\n",
    "test_df = pd.read_csv(test_csv, index_col='Unnamed: 0')\n",
    "val_df = pd.read_csv(val_csv, index_col='Unnamed: 0')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57318e71-dc3c-4978-89a7-85387744bc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df['author'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "064962ec-eb85-40b9-bf7b-492665c54a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = train_df['subjects'].replace('set()',np.nan)\n",
    "subj_docs = []\n",
    "for h in subj:\n",
    "    try:\n",
    "        h = h.strip(\"{}\")[1:-1]\n",
    "    except AttributeError:\n",
    "        subj_docs.append(h)\n",
    "        continue\n",
    "    h = h.replace(' -- ', '-')\n",
    "    h = h.replace(\"', '\",\"_\")\n",
    "    h = h.split('_')\n",
    "    h = [item.replace(' ','').replace(',', ' ') for item in h]\n",
    "    h = ' '.join(h)\n",
    "    subj_docs.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f13d40b-fb10-4122-9503-3fdd0d49d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['subj_str']=subj_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ae85678-cbde-4f72-aa04-aa18687c7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df['subject_str'] = train_df['subjects'].apply(lambda x: split_subjects(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bab03ba0-50a4-4263-b70b-1e450df7746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3da0a2f0-29d6-4991-8432-05c24d333203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 5.520370244979858 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_df['text'] = train_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "test_df['text'] = test_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "val_df['text'] = val_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time elapsed: {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6010e4b5-6a68-4cc6-89cc-35a5bbd48c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the word, line and token counts\n",
    "def enrich_dataframe(df):\n",
    "    count_path = os.path.join(gutenberg_repo_path, 'data', 'counts')\n",
    "    text_path = os.path.join(gutenberg_repo_path, 'data', 'text')\n",
    "    token_path = os.path.join(gutenberg_repo_path, 'data', 'tokens')\n",
    "\n",
    "    df['word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_word_count(pid, count_path))\n",
    "    df['unique_word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_unique_word_count(pid, count_path))\n",
    "    df['line_count'] = df['id'].apply(lambda pid: dataset_filtering.get_line_count(pid, text_path))\n",
    "    df['token_count'] = df['id'].apply(lambda pid: dataset_filtering.get_token_count(pid, token_path))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c89ef0ae-0fef-4ada-9378-034d2890a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = enrich_dataframe(train_df)\n",
    "val_df = enrich_dataframe(val_df)\n",
    "test_df = enrich_dataframe(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31a62839-5800-480c-9448-b157e495c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_start_and_end(text):\n",
    "    text = text.split(' ')\n",
    "    text = text[50:-50]\n",
    "    return ' '.join(text)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(skip_start_and_end)\n",
    "test_df['text'] = test_df['text'].apply(skip_start_and_end)\n",
    "val_df['text'] = val_df['text'].apply(skip_start_and_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "def85af7-de98-44b6-8183-71c55a8996c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_chunks(text, num_chunks=10, chunk_size=1000, overlap=False):\n",
    "    chunk = []\n",
    "    words = text.split(' ')\n",
    "\n",
    "    if num_chunks * chunk_size > len(words):\n",
    "        return text\n",
    "    for i in range(num_chunks):\n",
    "        new_words = []\n",
    "        num_words = len(words)\n",
    "        if chunk_size > num_words:\n",
    "            chunk = chunk + words\n",
    "            words = []\n",
    "            return ' '.join(chunk)\n",
    "        start = random.randint(0, num_words)\n",
    "        chunk = [*chunk,  *words[start:start+chunk_size]]\n",
    "        #print(chunk)\n",
    "        if start == 0:\n",
    "            words = words[chunk_size:]\n",
    "        elif start == num_words - chunk_size:\n",
    "            words = words[0:start]\n",
    "        else:\n",
    "            words = words[0:start] + words[start+chunk_size:]\n",
    "    return ' '.join(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6d6c419-fb8d-4aaf-80a6-a1905fc19f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['chunks'] = train_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 500, overlap=False))\n",
    "test_df['chunks'] = test_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 500, overlap=False))\n",
    "val_df['chunks'] = val_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 500, overlap=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d3a0452-7502-4ee8-ac66-8abab76c420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 7.0299530029296875 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    train_df['tokenized'] = pool.map(word_tokenize, train_df['chunks'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8747fa30-c79a-4445-b89f-3bdad5d72ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2.0713865756988525 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    val_df['tokenized'] = pool.map(word_tokenize, val_df['chunks'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee8b788e-bad3-4a35-8958-45de059976b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2.064713716506958 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    test_df['tokenized'] = pool.map(word_tokenize, test_df['chunks'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "095b07d6-4101-47ae-bf35-78f4afd85645",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'train_df_chunks_tokenized.pkl')\n",
    "train_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'val_df_chunks_tokenized.pkl')\n",
    "val_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'test_df_chunks_tokenized.pkl')\n",
    "test_df.to_pickle(outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0c4a0-7ea1-4336-bc3f-1eb2eef98be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bb95786-4686-49f4-987c-3ba5f033ea18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>subj_str</th>\n",
       "      <th>text</th>\n",
       "      <th>chunks</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, author, authoryearofbirth, authoryearofdeath, language, downloads, subjects, subj_str, text, chunks, tokenized]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5754768-e015-4dd4-9cad-edf72f6e8c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>text</th>\n",
       "      <th>chunks</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0.1, id, title, author, authoryearofbirth, authoryearofdeath, language, downloads, subjects, text, chunks, tokenized]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c735553-42b1-4b6b-826c-08e16c6612cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>text</th>\n",
       "      <th>chunks</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0.1, id, title, author, authoryearofbirth, authoryearofdeath, language, downloads, subjects, text, chunks, tokenized]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[val_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2795859-7ed7-4d9f-aa64-d2aab1f0af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['author'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b727fad-3b10-48db-aef3-02173eb4b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03026a4a-dd24-4f93-bbdc-165219447a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(tokenized_text):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(tokenized_text):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_final = word_lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            final_words.append(word_final)\n",
    "    return str(final_words)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2113b336-d875-4362-8432-f715578960d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    lemmatized = pool.map(lemmatize_text, train_df['tokenized'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "train_df['lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa10616-d82b-41ce-9025-637f9c7d3d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    lemmatized = pool.map(lemmatize_text, val_df['tokenized'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "val_df['lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccdb7d2-2c4b-48ea-9452-1e60d2095f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    lemmatized = pool.map(lemmatize_text, test_df['tokenized'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "test_df['lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f973e-84d2-4f2e-ac97-067e9cf3c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('text', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "val_df.drop('text', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "test_df.drop('text', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bd53cea7-6ccb-4cea-b25f-72f347e5cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'train_df_10chunks1000_Lemmatized.pkl')\n",
    "train_df=pd.read_pickle(outfile)\n",
    "#train_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'val_df_10chunks1000_Lemmatized.pkl')\n",
    "val_df= pd.read_pickle(outfile)\n",
    "#val_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'test_df_10chunks1000_Lemmatized.pkl')\n",
    "test_df  = pd.read_pickle(outfile)\n",
    "#test_df.to_pickle(outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5da91f-56e4-418a-9d91-3663f3b95b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "909eb400-a0af-4765-a776-05ee5c1c324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Train_Y = train_df['lemmatized'], train_df['author']\n",
    "Test_X, Test_Y = test_df['lemmatized'], test_df['author']\n",
    "Val_X, Val_Y = val_df['lemmatized'], val_df['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed26a1-cdfd-49f3-a059-3508fc3037d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "444b2c5d-5a8a-4d1a-a186-bf9bdc8c518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LabelEncoder()\n",
    "# Train_Y_e = Encoder.fit_transform(Train_Y)\n",
    "# Test_Y_e = Encoder.fit_transform(Test_Y)\n",
    "# Val_Y_e = Encoder.fit_transform(Val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9c14031f-b2b1-4d6d-91d1-0684ec6b210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=15000)\n",
    "Tfidf_vect.fit(train_df['lemmatized'])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "Val_X_Tfidf = Tfidf_vect.transform(Val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1da9f0d-a9d2-4655-8871-4d5300599242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, res_file, model_description, preproc_desc):\n",
    "    # Train and predict\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Training and predicting took {end-start} seconds = {(end-start)/60} minutes')\n",
    "\n",
    "    results={}\n",
    "    for label, y_truth, y_pred in [('train', y_train, y_train_pred), \n",
    "                            ('validation', y_val, y_val_pred),\n",
    "                            ('test', y_test, y_test_pred)]:\n",
    "        # Metrics (set zero_division=0 to silence warnings)\n",
    "        acc = accuracy_score(y_truth, y_pred)\n",
    "        f1 = f1_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "        precision = precision_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        result_dict = {'accuracy': acc,\n",
    "                       'precision': precision,\n",
    "                       'recall' : recall,\n",
    "                       'f1' : f1}\n",
    "        results[label] = result_dict\n",
    "        \n",
    "        \n",
    "    # Print performance\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    label_str=f'|{'':<15} ||  {'Accuracy':>15} | {'Precision':>15} | {'Recall':>15} | {'F1-Score':>15} |'\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    print(label_str)\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    for result_label, sub_res_dict in results.items():\n",
    "        output_str = f'|{result_label:<15} || '\n",
    "        \n",
    "        for key, val in sub_res_dict.items():\n",
    "            output_str += f' {val:15.4f} |'\n",
    "        print(output_str)\n",
    "\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    new_res_df = results_to_df(model.__class__.__name__, model_description, preproc_desc, results)\n",
    "    \n",
    "    if os.path.exists(res_file):\n",
    "        old_res_df = pd.read_csv(res_file)\n",
    "        old_res_df.set_index(['model_type', 'description', 'preprocessing description', 'metric'], inplace=True)\n",
    "    \n",
    "        res_df = pd.concat([old_res_df, new_res_df])\n",
    "        res_df.to_csv(res_file)\n",
    "    else:\n",
    "        new_res_df.to_csv(res_file)\n",
    "        \n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "96722785-e12f-49a0-8086-93c54df4d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(model_type, model_desc, preproc_desc, result_dict):\n",
    "    res_df = pd.DataFrame.from_dict(result_dict)\n",
    "    res_df['model_type'] = model_type\n",
    "    res_df['description'] = model_desc\n",
    "    res_df['preprocessing description'] = preproc_desc\n",
    "    res_df.reset_index(inplace=True)\n",
    "    res_df.rename({'index':'metric'}, axis=1, inplace=True)\n",
    "    res_df.set_index(['model_type', 'description', 'preprocessing description', 'metric'], inplace=True)\n",
    "\n",
    "    return res_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472d1f8-19a0-417f-85b7-dbcb1cb48fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "513c3140-80ec-4623-8e30-5aa535b865e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=os.path.join(gutenberg_corpus_analysis_repo,'results', 'MLP.csv')\n",
    "preprocessing_description = '10 chunks, 1000 long, tf-idf 15000 max feat'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ca023-f4ae-4651-9afc-bb70b01d3eb2",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d96d3bf-44aa-4e04-ac38-9b34d01771fc",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9cd9f9-c304-4a79-878d-ae628000223c",
   "metadata": {},
   "source": [
    "## 1 Hidden Layer, sizes 100, 200, 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "16fd870f-1add-4c0b-b25a-dc7a88339dc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 75.8631980419159 seconds = 1.2643866340319316 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9000 |          0.9156 |          0.9000 |          0.8987 |\n",
      "|test            ||           0.9042 |          0.9194 |          0.9042 |          0.8982 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier()\n",
    "model_desc='default'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "40ea159e-099e-421b-8660-cb32f10614f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 125.62376499176025 seconds = 2.0937294165293374 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9125 |          0.9277 |          0.9125 |          0.9102 |\n",
      "|test            ||           0.9125 |          0.9237 |          0.9125 |          0.9071 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,))\n",
    "model_desc='hidden_layer_size 200'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "962aaf04-88db-4e69-8f9d-c144d13f5328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 296.95596957206726 seconds = 4.949266159534455 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9042 |          0.9231 |          0.9042 |          0.9013 |\n",
      "|test            ||           0.9125 |          0.9129 |          0.9125 |          0.9036 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,))\n",
    "model_desc='hidden_layer_size 500'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125c060-dea2-4dde-842e-9ccd7cc2cd31",
   "metadata": {},
   "source": [
    "## 2 Hidden Layers, sizes 100, 200, 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cdabdce5-da8f-454b-8d07-73264c229daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 50.64549684524536 seconds = 0.8440916140874227 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8583 |          0.8925 |          0.8583 |          0.8575 |\n",
      "|test            ||           0.8708 |          0.8837 |          0.8708 |          0.8608 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(100,100,))\n",
    "model_desc='2 hidden_layer_size 100'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9498133f-c5a5-4c86-ae03-dff1ca6d24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 89.51257276535034 seconds = 1.491876212755839 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8917 |          0.9131 |          0.8917 |          0.8904 |\n",
      "|test            ||           0.8833 |          0.9029 |          0.8833 |          0.8785 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,200,))\n",
    "model_desc='2 hidden_layer_size 200'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "49e4ab1c-f9d6-4287-867b-a6f91511fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 202.14327549934387 seconds = 3.3690545916557313 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8750 |          0.8992 |          0.8750 |          0.8726 |\n",
      "|test            ||           0.9000 |          0.9131 |          0.9000 |          0.8921 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,500,))\n",
    "model_desc='2 hidden_layer_size 500'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2780c-8b05-485b-86b9-dd4872f11f67",
   "metadata": {},
   "source": [
    "## 3 Hidden Layers, sizes 100, 200, 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b21d3db3-065e-4b41-9b69-550c8bf32390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 40.618685722351074 seconds = 0.6769780953725179 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.7792 |          0.8392 |          0.7792 |          0.7762 |\n",
      "|test            ||           0.7917 |          0.8172 |          0.7917 |          0.7796 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(100,100,100,))\n",
    "model_desc='3 hidden_layer_size 100'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c483d190-9b9d-4223-ba48-d8143fc7cd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 63.55175423622131 seconds = 1.0591959039370218 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.7958 |          0.8395 |          0.7958 |          0.7894 |\n",
      "|test            ||           0.8042 |          0.8554 |          0.8042 |          0.7998 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,200,200,))\n",
    "model_desc='3 hidden_layer_size 200'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1ff6402d-65df-40ab-8bbd-f50225c7b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 154.4944007396698 seconds = 2.5749066789944965 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8583 |          0.8971 |          0.8583 |          0.8599 |\n",
      "|test            ||           0.8417 |          0.8660 |          0.8417 |          0.8305 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,500,500,))\n",
    "model_desc='3 hidden_layer_size 500'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1811a180-23da-4b19-a1f7-f6267eb7664a",
   "metadata": {},
   "source": [
    "## Miscellaneous networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "12f790fd-1f76-499f-9d1a-f483d72794ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 99.63221025466919 seconds = 1.6605368375778198 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9167 |          0.9344 |          0.9167 |          0.9151 |\n",
      "|test            ||           0.9042 |          0.9162 |          0.9042 |          0.8997 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(activation='logistic', max_iter=500)\n",
    "model_desc='logistic act, 500 iter'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "754ff6d0-b720-4d09-bba4-e0b5ca2ceb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 273.6840138435364 seconds = 4.5614002307256065 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9083 |          0.9356 |          0.9083 |          0.9081 |\n",
      "|test            ||           0.9250 |          0.9350 |          0.9250 |          0.9187 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,), activation='logistic', max_iter=500)\n",
    "model_desc='hidden layer size 200, logistic act, 500 iter'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "d8fa8c00-b818-475c-be3e-70d14924eb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 8.986948013305664 seconds = 0.14978246688842772 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "|validation      ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "|test            ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,), activation='logistic', max_iter=500, learning_rate='invscaling', solver='sgd')\n",
    "model_desc='hidden layer size 200, logistic act, 500 iter, sgd-invscaling'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "3ad3df5a-13d1-48f6-9700-c3b0c99f50f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 21.707059860229492 seconds = 0.36178433100382484 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0109 |          0.0005 |          0.0109 |          0.0009 |\n",
      "|validation      ||           0.0125 |          0.0005 |          0.0125 |          0.0010 |\n",
      "|test            ||           0.0083 |          0.0002 |          0.0083 |          0.0004 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,), activation='relu', max_iter=500, learning_rate='invscaling', solver='sgd')\n",
    "model_desc='hidden layer size 500, relu act, 500 iter, sgd-invscaling'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "29c74b1e-2c00-4bad-95f0-52e77d0352a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dean/anaconda3/envs/school-env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 851.5928704738617 seconds = 14.193214507897695 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.4568 |          0.7121 |          0.4568 |          0.4622 |\n",
      "|validation      ||           0.3250 |          0.3742 |          0.3250 |          0.3133 |\n",
      "|test            ||           0.3375 |          0.3766 |          0.3375 |          0.3095 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,), activation='relu', max_iter=500, learning_rate='constant', solver='sgd')\n",
    "model_desc='hidden layer size 500, relu act, 500 iter, sgd-constant'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "6eb0f85d-3eb2-4d65-ba15-b2a5ba17c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 185.4526436328888 seconds = 3.0908773938814798 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9292 |          0.9440 |          0.9292 |          0.9265 |\n",
      "|test            ||           0.9167 |          0.9225 |          0.9167 |          0.9103 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 500, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "e684cc06-e386-4b98-b14c-d9b951cca409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 23.782085180282593 seconds = 0.3963680863380432 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0146 |          0.0004 |          0.0146 |          0.0007 |\n",
      "|validation      ||           0.0083 |          0.0002 |          0.0083 |          0.0004 |\n",
      "|test            ||           0.0083 |          0.0002 |          0.0083 |          0.0005 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,200, 100, ), activation='relu', max_iter=500, learning_rate='invscaling', solver='sgd')\n",
    "model_desc='hidden layer size 500-200-100, relu act, 500 iter, sgd-invscaling'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "efbd3524-71f6-45c4-8e91-8a7f62d65bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 138.3572220802307 seconds = 2.3059537013371787 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0198 |          0.0008 |          0.0198 |          0.0015 |\n",
      "|validation      ||           0.0083 |          0.0004 |          0.0083 |          0.0007 |\n",
      "|test            ||           0.0292 |          0.0013 |          0.0292 |          0.0024 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,200, 100, ), activation='relu', max_iter=500, learning_rate='constant', solver='sgd')\n",
    "model_desc='hidden layer size 500-200-100, relu act, 500 iter, sgd-constant'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "e62731c6-bdd5-4990-83c8-1bcfa81f9ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 127.03732872009277 seconds = 2.1172888120015463 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8833 |          0.9142 |          0.8833 |          0.8776 |\n",
      "|test            ||           0.8375 |          0.8545 |          0.8375 |          0.8272 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,200, 100, ), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 500-200-100, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "a51dfd96-f761-4149-920a-29e1a9c1573a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 8.672466516494751 seconds = 0.14454110860824584 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0135 |          0.0008 |          0.0135 |          0.0013 |\n",
      "|validation      ||           0.0167 |          0.0010 |          0.0167 |          0.0017 |\n",
      "|test            ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,100, ), activation='relu', max_iter=500, learning_rate='invscaling', solver='sgd')\n",
    "model_desc='hidden layer size 200-100, relu act, 500 iter, sgd-invscaling'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "1b03d26a-3ccc-4e06-985a-c8fde3bde853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 67.76165580749512 seconds = 1.1293609301249186 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8958 |          0.9117 |          0.8958 |          0.8930 |\n",
      "|test            ||           0.9000 |          0.9237 |          0.9000 |          0.8950 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,100, ), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 200-100, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "883745bc-d091-4cae-9c61-77b9c48753fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 8.011568307876587 seconds = 0.13352613846460978 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "|validation      ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "|test            ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,200, ), activation='relu', max_iter=500, learning_rate='invscaling', solver='sgd')\n",
    "model_desc='hidden layer size 200-200, relu act, 500 iter, sgd-invscaling'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "6d36d340-b0c0-4a2e-b776-23fbb6318517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 27.243923902511597 seconds = 0.4540653983751933 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0135 |          0.0004 |          0.0135 |          0.0007 |\n",
      "|validation      ||           0.0125 |          0.0003 |          0.0125 |          0.0006 |\n",
      "|test            ||           0.0125 |          0.0003 |          0.0125 |          0.0006 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,200, ), activation='relu', max_iter=500, learning_rate='constant', solver='sgd')\n",
    "model_desc='hidden layer size 200-200, relu act, 500 iter, sgd-constant'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "6be55c5d-4d6c-4df8-9b03-cc215be39747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 58.8208589553833 seconds = 0.9803476492563884 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8792 |          0.8996 |          0.8792 |          0.8734 |\n",
      "|test            ||           0.8750 |          0.8904 |          0.8750 |          0.8653 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,200, ), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 200-200, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "32eccf56-cf9e-4fc9-b9d1-6ab48d67aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 28.916454076766968 seconds = 0.48194090127944944 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8833 |          0.9079 |          0.8833 |          0.8819 |\n",
      "|test            ||           0.8792 |          0.8940 |          0.8792 |          0.8689 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(100,50, ), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 100-50, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "a29c6686-0928-4a1d-a097-c6411abad82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 33.85341835021973 seconds = 0.5642236391703288 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8292 |          0.8498 |          0.8292 |          0.8212 |\n",
      "|test            ||           0.8875 |          0.9099 |          0.8875 |          0.8815 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(100,100, ), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 100-100, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "6d368166-9fee-4861-b5df-353072877fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 25.500365018844604 seconds = 0.42500608364741005 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8292 |          0.8617 |          0.8292 |          0.8239 |\n",
      "|test            ||           0.8208 |          0.8352 |          0.8208 |          0.8115 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(100,100, 100,), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 100-100-100, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fea38d-dc9c-4de2-9fdb-54f3fcd4f875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_school_env",
   "language": "python",
   "name": "school_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
