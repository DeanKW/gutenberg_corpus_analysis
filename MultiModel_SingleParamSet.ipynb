{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0425b546-d293-43b4-8bd5-c39d5f551289",
   "metadata": {},
   "source": [
    "# Use this script to quickly test many model types against a single set of parameters\n",
    "\n",
    "Currently supports only lemmatized\n",
    "Will later support: stemming, no stemming/lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ad041-5d30-4980-969c-4c19a61ecfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Set these parameters\n",
    "######################\n",
    "\n",
    "# If chunking is set to False, the full text is used\n",
    "CHUNKING = True\n",
    "#####\n",
    "# Only used if chunking is true\n",
    "NUM_CHUNKS = 10\n",
    "CHUNK_SIZE = 100\n",
    "CHUNK_OVERLAP=False\n",
    "#####\n",
    "# TF-IDF Parameters\n",
    "TF_IDF_MAX_FEATURES=15000\n",
    "TF_IDF_N_GRAM=(1,3)\n",
    "\n",
    "LEMMATIZATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b1aef-e70b-4538-8301-a3e3c4ea39c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95429125-6109-4627-ac76-b6f28500064d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2f49a-fdd3-41a5-b9c9-a111ac1c126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm, neural_network\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, make_scorer)\n",
    "import multiprocessing as mp\n",
    "\n",
    "from src import data_loader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924525d-c6a6-403f-8db8-18484651cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd602c87-cec2-40b9-8f87-9e2452677907",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_repo_path = os.path.join(repos_path, 'gutenberg')\n",
    "gutenberg_analysis_repo = os.path.join(repos_path, 'gutenberg-analysis')\n",
    "\n",
    "src_dir = os.path.join(gutenberg_analysis_repo,'src')\n",
    "sys.path.append(src_dir)\n",
    "from data_io import get_book\n",
    "\n",
    "\n",
    "gutenberg_src_dir = os.path.join(gutenberg_repo_path,'src')\n",
    "sys.path.append(gutenberg_src_dir)\n",
    "\n",
    "from metaquery import meta_query\n",
    "\n",
    "gca_path = os.path.abspath(os.getcwd())\n",
    "sys.path.append(gca_path)\n",
    "import misc_utils.dataset_filtering as dataset_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd0659c-f138-4831-8d41-b8e368c72d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='nikita_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfe074-d977-4894-97ba-1afb2669943c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba801a-3aa7-499c-8c80-d10ce5e6faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wish to specify the file locations of the DFS:\n",
    "# load_dfs takes arguments dataset_folder, train_csv, val_csv and test_csv\n",
    "train_df, val_df, test_df = data_loader.load_dfs(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1d883-c41a-48c8-b52a-ba765b8a4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = train_df['subjects'].replace('set()',np.nan)\n",
    "subj_docs = []\n",
    "for h in subj:\n",
    "    try:\n",
    "        h = h.strip(\"{}\")[1:-1]\n",
    "    except AttributeError:\n",
    "        subj_docs.append(h)\n",
    "        continue\n",
    "    h = h.replace(' -- ', '-')\n",
    "    h = h.replace(\"', '\",\"_\")\n",
    "    h = h.split('_')\n",
    "    h = [item.replace(' ','').replace(',', ' ') for item in h]\n",
    "    h = ' '.join(h)\n",
    "    subj_docs.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8cbaf-aac4-497b-b325-88fadd9668a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['subj_str']=subj_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9b802-185d-400a-9c81-861f4a602509",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "test_df['text'] = test_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "val_df['text'] = val_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c4d9b-0925-41e3-9124-c06fcad55f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the word, line and token counts\n",
    "def enrich_dataframe(df):\n",
    "    count_path = os.path.join(gutenberg_repo_path, 'data', 'counts')\n",
    "    text_path = os.path.join(gutenberg_repo_path, 'data', 'text')\n",
    "    token_path = os.path.join(gutenberg_repo_path, 'data', 'tokens')\n",
    "\n",
    "    df['word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_word_count(pid, count_path))\n",
    "    df['unique_word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_unique_word_count(pid, count_path))\n",
    "    df['line_count'] = df['id'].apply(lambda pid: dataset_filtering.get_line_count(pid, text_path))\n",
    "    df['token_count'] = df['id'].apply(lambda pid: dataset_filtering.get_token_count(pid, token_path))\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = enrich_dataframe(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b832cc8-01a9-452e-aae9-a0c3e0129e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2974d-976d-4612-89c9-a32f9a159704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55835d2b-b3fc-4924-8843-461890af1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_start_and_end(text, num_chars=100):\n",
    "    text = text.split(' ')\n",
    "    text = text[num_chars:-num_chars]\n",
    "    return ' '.join(text)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(skip_start_and_end)\n",
    "test_df['text'] = test_df['text'].apply(skip_start_and_end)\n",
    "val_df['text'] = val_df['text'].apply(skip_start_and_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c21af4-ad94-469a-820a-9e019b59005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_chunks(text, num_chunks=10, chunk_size=1000, overlap=False):\n",
    "    chunk = []\n",
    "    words = text.split(' ')\n",
    "\n",
    "    if num_chunks * chunk_size > len(words):\n",
    "        return text\n",
    "    for i in range(num_chunks):\n",
    "        new_words = []\n",
    "        num_words = len(words)\n",
    "        if chunk_size > num_words:\n",
    "            chunk = chunk + words\n",
    "            words = []\n",
    "            return ' '.join(chunk)\n",
    "        start = random.randint(0, num_words)\n",
    "        chunk = [*chunk,  *words[start:start+chunk_size]]\n",
    "        #print(chunk)\n",
    "        if start == 0:\n",
    "            words = words[chunk_size:]\n",
    "        elif start == num_words - chunk_size:\n",
    "            words = words[0:start]\n",
    "        else:\n",
    "            words = words[0:start] + words[start+chunk_size:]\n",
    "    return ' '.join(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd449f-17e8-4077-804b-0efa69e0b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHUNKING is True:\n",
    "\n",
    "    train_df['text'] = train_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=NUM_CHUNKS, chunk_size = CHUNK_SIZE, overlap=CHUNK_OVERLAP))\n",
    "    test_df['text'] = test_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=NUM_CHUNKS, chunk_size = CHUNK_SIZE, overlap=CHUNK_OVERLAP))\n",
    "    val_df['text'] = val_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=NUM_CHUNKS, chunk_size = CHUNK_SIZE, overlap=CHUNK_OVERLAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1805ac5-f707-4725-b80d-7640e5f1682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_col = 'text'\n",
    "tokenized_col = 'tokenized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc1fda-bcca-4e40-95e8-d3c59a7318cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df[tokenized_col] = process_map(word_tokenize, train_df[x_col], max_workers=11, chunksize=5)\n",
    "test_df[tokenized_col] = process_map(word_tokenize, test_df[x_col], max_workers=11, chunksize=5)\n",
    "val_df[tokenized_col] = process_map(word_tokenize, val_df[x_col], max_workers=11, chunksize=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c742a842-1741-4303-99b5-122e6ce79a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokenized'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770c49d-a510-447e-9609-6b7fa7554b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_df['tokenized'].isnull().any():\n",
    "    print('Warning: There are null elements in train_df')\n",
    "\n",
    "if val_df['tokenized'].isnull().any():\n",
    "    print('Warning: There are null elements in val_df')\n",
    "\n",
    "if test_df['tokenized'].isnull().any():\n",
    "    print('Warning: There are null elements in test_df')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549cac0-a21a-46ef-96bf-78c02d7157e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the tokenized full text, so you don't have to run this again later\n",
    "# if you so desire\n",
    "\n",
    "outfile=os.path.join(gca_path, 'tokenized', 'train_df_full_text_tokenized.pkl')\n",
    "train_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'tokenized', 'test_df_full_text_tokenized.pkl')\n",
    "test_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'tokenized', 'val_df_full_text_tokenized.pkl')\n",
    "val_df.to_pickle(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2617a-7ceb-4c30-a356-9cb723fcb537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab290ac-4ee4-4364-aeee-317508bd59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_that_thang(tokenized_text):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing PorterStemmer()\n",
    "    ps = PorterStemmer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word in tokenized_text:\n",
    "        if word not in stopwords.words('english'):\n",
    "            word_final = ps.stem(word)\n",
    "            final_words.append(word_final)\n",
    "\n",
    "    return str(final_words)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb5fba-2b33-40e7-9157-7c53e47a4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_that_thang(tokenized_text):\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV  \n",
    "    \n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(tokenized_text):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_final = word_lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            final_words.append(word_final)\n",
    "    return str(final_words)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be7b25-5a08-4cc6-92cc-c6b93fb3cca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d478cae6-258e-46fd-b5ab-8848b078a1b1",
   "metadata": {},
   "source": [
    "## Lemmatize the text and save out the DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a469e97-3c60-45ea-a691-a3c3a859fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['lemmatized'] = process_map(lemmatize_that_thang, train_df['tokenized'], max_workers=10, chunksize=5)\n",
    "val_df['lemmatized'] = process_map(lemmatize_that_thang, val_df['tokenized'], max_workers=10, chunksize=5)\n",
    "test_df['lemmatized'] = process_map(lemmatize_that_thang, test_df['tokenized'], max_workers=10, chunksize=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef55e45-3a25-43f2-9af0-960296f6737e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23aad2-cb3f-4631-b50c-e5d8fef3b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'train_df_full_text_lemmatized.pkl')\n",
    "train_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'val_df_full_text_lemmatized.pkl')\n",
    "val_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'test_df_full_text_lemmatized.pkl')\n",
    "test_df.to_pickle(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4c616-3c04-4ce4-b3ea-bf9ea4944067",
   "metadata": {},
   "source": [
    "# Read in the lemmatized texts, if you have already saved them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043db7ed-ef03-490b-98cf-d1ba0a212fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pkl = os.path.join(gca_path, 'nikita_dataset', 'train_df_full_text_lemmatized.pkl')\n",
    "val_pkl = os.path.join(gca_path, 'nikita_dataset', 'val_df_full_text_lemmatized.pkl')\n",
    "test_pkl = os.path.join(gca_path, 'nikita_dataset', 'test_df_full_text_lemmatized.pkl')\n",
    "\n",
    "train_df = pd.read_pickle(train_pkl)\n",
    "val_df = pd.read_pickle(val_pkl)\n",
    "test_df = pd.read_pickle(test_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b2505-7e3b-473b-b0f8-dabad1abb190",
   "metadata": {},
   "source": [
    "## Save a version of the dataframe without the text column, for memory reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b0804-3085-43e9-80cc-07f07432d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('text', axis=1, inplace=True)\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'train_df_full_text_lemmatized_noRawText.pkl')\n",
    "train_df.to_pickle(outfile)\n",
    "\n",
    "val_df.drop('text', axis=1, inplace=True)\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'val_df_full_text_lemmatized_noRawText.pkl')\n",
    "val_df.to_pickle(outfile)\n",
    "\n",
    "test_df.drop('text', axis=1, inplace=True)\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'test_df_full_text_lemmatized_noRawText.pkl')\n",
    "test_df.to_pickle(outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e28b28-fb9a-4051-8957-a819cd62581e",
   "metadata": {},
   "source": [
    "## Drop both the tokenization and text columns, for memory reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db751629-f988-4912-be0c-29ca1e6b3b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['tokenized', 'text'], axis=1, inplace=True)\n",
    "val_df.drop(['tokenized', 'text'], axis=1, inplace=True)\n",
    "test_df.drop(['tokenized', 'text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002e603-7fe9-4d93-b646-7c40bf1da966",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'test_df_full_text_lemmatizedOnly.pkl')\n",
    "train_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'val_df_full_text_lemmatizedOnly.pkl')\n",
    "val_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'test_df_full_text_lemmatizedOnly.pkl')\n",
    "test_df.to_pickle(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e30cd4-acc3-4cf5-b66c-21abadbb0dd7",
   "metadata": {},
   "source": [
    "# Lemmatization, English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0f61a-56cb-4aeb-8d7c-2b630cb8984e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ccc12-fa49-4a4d-9625-651580ad66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_col = 'tokenized'\n",
    "x_col='lemmatized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a12718-8895-461c-8df8-268fdf3db00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Train_Y = train_df[x_col], train_df['author']\n",
    "Test_X, Test_Y = test_df[x_col], test_df['author']\n",
    "Val_X, Val_Y = val_df[x_col], val_df['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1181cc9-c4da-4287-bcc1-a055c949db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y_e = Encoder.fit_transform(Train_Y)\n",
    "Test_Y_e = Encoder.fit_transform(Test_Y)\n",
    "Val_Y_e = Encoder.fit_transform(Val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352ea69-0a7b-44d3-8453-e2614872fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Tfidf_vect = TfidfVectorizer(\n",
    "    stop_words='english', # Removes a lot of common english words like it, and, that, is etc. Uses predifined scikit list of common english words.\n",
    "    sublinear_tf=True, # Uses logarithmic word frequency weighting, reducing the weight of extremely frequent terms & helps prevent domination by larger text files\n",
    "    max_features=TF_IDF_MAX_FEATURES, # Consideration for both overfitting and computational requirements.\n",
    "    ngram_range=TF_IDF_N_GRAM\n",
    ")\n",
    "Tfidf_vect.fit_transform(train_df[x_col])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "Val_X_Tfidf = Tfidf_vect.transform(Val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f5467-8618-4b47-a5e1-c676878f9dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda7617-0a96-47f0-b648-4dfab9dded78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, res_file, model_description, preproc_desc):\n",
    "    # Train and predict\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Training and predicting took {end-start} seconds = {(end-start)/60} minutes')\n",
    "\n",
    "    results={}\n",
    "    for label, y_truth, y_pred in [('train', y_train, y_train_pred), \n",
    "                            ('validation', y_val, y_val_pred),\n",
    "                            ('test', y_test, y_test_pred)]:\n",
    "        # Metrics (set zero_division=0 to silence warnings)\n",
    "        acc = accuracy_score(y_truth, y_pred)\n",
    "        f1 = f1_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "        precision = precision_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        result_dict = {'accuracy': acc,\n",
    "                       'precision': precision,\n",
    "                       'recall' : recall,\n",
    "                       'f1' : f1}\n",
    "        results[label] = result_dict\n",
    "        \n",
    "        \n",
    "    # Print performance\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f'Description: {model_description}')\n",
    "    print(f'Pre-processing: {preproc_desc}')\n",
    "    label_str=f'|{'':<15} ||  {'Accuracy':>15} | {'Precision':>15} | {'Recall':>15} | {'F1-Score':>15} |'\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    print(label_str)\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    for result_label, sub_res_dict in results.items():\n",
    "        output_str = f'|{result_label:<15} || '\n",
    "        \n",
    "        for key, val in sub_res_dict.items():\n",
    "            output_str += f' {val:15.4f} |'\n",
    "        print(output_str)\n",
    "\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    new_res_df = results_to_df(model.__class__.__name__, model_description, preproc_desc, results)\n",
    "    \n",
    "    if os.path.exists(res_file):\n",
    "        old_res_df = pd.read_csv(res_file)\n",
    "        old_res_df.set_index(['model_type', 'description', 'preprocessing description', 'metric'], inplace=True)\n",
    "    \n",
    "        res_df = pd.concat([old_res_df, new_res_df])\n",
    "        res_df.to_csv(res_file)\n",
    "    else:\n",
    "        new_res_df.to_csv(res_file)\n",
    "        \n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e45d8-7c1b-4e40-9678-174f11c5ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(model_type, model_desc, preproc_desc, result_dict):\n",
    "    res_df = pd.DataFrame.from_dict(result_dict)\n",
    "    res_df['model_type'] = model_type\n",
    "    res_df['description'] = model_desc\n",
    "    res_df['preprocessing description'] = preproc_desc\n",
    "    res_df.reset_index(inplace=True)\n",
    "    res_df.rename({'index':'metric'}, axis=1, inplace=True)\n",
    "    res_df.set_index(['model_type', 'description', 'preprocessing description', 'metric'], inplace=True)\n",
    "\n",
    "    return res_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80c46e-2a80-4181-b4a5-a850dfed8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_those_models(Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, preprocessing_description):\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.SVC()\n",
    "    model_desc = 'default_settings'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    model_desc = 'C=1, kernel Linear, deg 3, gamma auto'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # No Effect\n",
    "    # # Classifier - Algorithm - SVM\n",
    "    # model = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='scale')\n",
    "    # model_desc='C=1, kernel Linear, deg 3 gamma scale'\n",
    "    # model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "    #                                     outfile, model_desc, preprocessing_description)\n",
    "\n",
    "\n",
    "    # Usually poor results\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # model = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto')\n",
    "    # model_desc='C=1, kernel rbf, deg 3 gamma auto'\n",
    "    # model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "    #                                     outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='scale')\n",
    "    model_desc='C=1, kernel rbf, deg 3 gamma scale'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='poly', degree=4, gamma='scale')\n",
    "    model_desc='C=1, kernel poly, deg 4 gamma scale'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='poly', degree=2, gamma='scale')\n",
    "    model_desc='C=1, kernel poly, deg 2 gamma scale'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='scale')\n",
    "    model_desc='C=1, kernel poly, deg 3 gamma scale'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # usually poor results\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # model = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto')\n",
    "    # model_desc='C=1, kernel poly, deg 3 gamma auto'\n",
    "    # model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "    #                                     outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='scale')\n",
    "    model_desc='C=1, kernel sigmoid, deg 3 gamma scale'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Usually poor results\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    # model = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto')\n",
    "    # model_desc='C=1, kernel sigmoid, deg 3 gamma auto'\n",
    "    # model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "    #                                     outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC()\n",
    "    model_desc='default'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=10)\n",
    "    model_desc='C=10'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=10, max_iter=5000)\n",
    "    model_desc='C=10, max_it 5000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=10, max_iter=50000)\n",
    "    model_desc='C=10, max_it 50000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=20, max_iter=5000)\n",
    "    model_desc='C=20, max_it 5000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=20, max_iter=50000)\n",
    "    model_desc='C=20, max_it 50000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=10, multi_class='crammer_singer', max_iter=5000)\n",
    "    model_desc='C=10, crammer-singer, max_it 5000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=10, multi_class='crammer_singer', max_iter=50000)\n",
    "    model_desc='C=10, crammer-singer, max_it 5000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=20, multi_class='crammer_singer', max_iter=5000)\n",
    "    model_desc='C=20, crammer-singer, max_it 5000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=20, multi_class='crammer_singer', max_iter=50000)\n",
    "    model_desc='C=20, crammer-singer, max_it 50000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.NuSVC()\n",
    "    model_desc='default'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # fit the training dataset on the NB classifier\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model_desc='default'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    model = neural_network.MLPClassifier()\n",
    "    model_desc='default'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    model = neural_network.MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000)\n",
    "    model_desc='hidden_layer_size 200, max it 1000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    model = neural_network.MLPClassifier(activation='logistic', max_iter=500)\n",
    "    model_desc='logistic act, 500 iter'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1458b-0aa2-41a5-b751-9053fd73429c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c373b-d86d-4ef3-a6ce-f9b7ba2733b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462af034-8085-4555-96a6-be409bd227a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9db37e-04d7-4e14-af73-9661e203e795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccb7ad-078f-45d1-be62-e02bcfd6862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(\n",
    "    stop_words='english', # Removes a lot of common english words like it, and, that, is etc. Uses predifined scikit list of common english words.\n",
    "    sublinear_tf=True, # Uses logarithmic word frequency weighting, reducing the weight of extremely frequent terms & helps prevent domination by larger text files\n",
    "    max_features=10000, # Consideration for both overfitting and computational requirements.\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "Tfidf_vect.fit_transform(train_df[x_col])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "Val_X_Tfidf = Tfidf_vect.transform(Val_X)\n",
    "\n",
    "outfile='/home/dean/Documents/gitRepos/gutenberg_corpus_analysis/SVM/results_full_text_1.csv'\n",
    "preprocessing_description='tf-idf sublinear_tf true, max feat 10000, ngram (1,2)'\n",
    "train_those_models(Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, preprocessing_description)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school-env",
   "language": "python",
   "name": "school-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
