{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0425b546-d293-43b4-8bd5-c39d5f551289",
   "metadata": {},
   "source": [
    "# Use this script to quickly test many model types against a single set of parameters\n",
    "\n",
    "Currently supports only full text and lemmatized and one set of tf-df params\n",
    "Will later support: chunking, stemming, no stemming/lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a2f49a-fdd3-41a5-b9c9-a111ac1c126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm, neural_network\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, make_scorer)\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c924525d-c6a6-403f-8db8-18484651cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd602c87-cec2-40b9-8f87-9e2452677907",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_repo_path = os.path.join(repos_path, 'gutenberg')\n",
    "gutenberg_analysis_repo = os.path.join(repos_path, 'gutenberg-analysis')\n",
    "\n",
    "src_dir = os.path.join(gutenberg_analysis_repo,'src')\n",
    "sys.path.append(src_dir)\n",
    "from data_io import get_book\n",
    "\n",
    "\n",
    "gutenberg_src_dir = os.path.join(gutenberg_repo_path,'src')\n",
    "sys.path.append(gutenberg_src_dir)\n",
    "\n",
    "from metaquery import meta_query\n",
    "\n",
    "gca_path = os.path.abspath(os.getcwd())\n",
    "sys.path.append(gca_path)\n",
    "import misc_utils.dataset_filtering as dataset_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd0659c-f138-4831-8d41-b8e368c72d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='nikita_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9df13c91-756a-487b-8518-3325308aaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = os.path.join(gca_path, dataset, 'final_train.csv')\n",
    "test_csv = os.path.join(gca_path, dataset, 'final_test.csv')\n",
    "val_csv = os.path.join(gca_path, dataset, 'final_val.csv')\n",
    "\n",
    "pg_catalog_filepath=os.path.join(gutenberg_repo_path, 'metadata', 'pg_catalog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb82ad9-9e70-4aa1-808c-810e980caff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv, index_col='Unnamed: 0')\n",
    "test_df = pd.read_csv(test_csv, index_col='Unnamed: 0')\n",
    "val_df = pd.read_csv(val_csv, index_col='Unnamed: 0')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1d883-c41a-48c8-b52a-ba765b8a4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = train_df['subjects'].replace('set()',np.nan)\n",
    "subj_docs = []\n",
    "for h in subj:\n",
    "    try:\n",
    "        h = h.strip(\"{}\")[1:-1]\n",
    "    except AttributeError:\n",
    "        subj_docs.append(h)\n",
    "        continue\n",
    "    h = h.replace(' -- ', '-')\n",
    "    h = h.replace(\"', '\",\"_\")\n",
    "    h = h.split('_')\n",
    "    h = [item.replace(' ','').replace(',', ' ') for item in h]\n",
    "    h = ' '.join(h)\n",
    "    subj_docs.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8cbaf-aac4-497b-b325-88fadd9668a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['subj_str']=subj_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9b802-185d-400a-9c81-861f4a602509",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "test_df['text'] = test_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "val_df['text'] = val_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c4d9b-0925-41e3-9124-c06fcad55f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the word, line and token counts\n",
    "def enrich_dataframe(df):\n",
    "    count_path = os.path.join(gutenberg_repo_path, 'data', 'counts')\n",
    "    text_path = os.path.join(gutenberg_repo_path, 'data', 'text')\n",
    "    token_path = os.path.join(gutenberg_repo_path, 'data', 'tokens')\n",
    "\n",
    "    df['word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_word_count(pid, count_path))\n",
    "    df['unique_word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_unique_word_count(pid, count_path))\n",
    "    df['line_count'] = df['id'].apply(lambda pid: dataset_filtering.get_line_count(pid, text_path))\n",
    "    df['token_count'] = df['id'].apply(lambda pid: dataset_filtering.get_token_count(pid, token_path))\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = enrich_dataframe(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b832cc8-01a9-452e-aae9-a0c3e0129e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2974d-976d-4612-89c9-a32f9a159704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55835d2b-b3fc-4924-8843-461890af1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_start_and_end(text, num_chars=100):\n",
    "    text = text.split(' ')\n",
    "    text = text[num_chars:-num_chars]\n",
    "    return ' '.join(text)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(skip_start_and_end)\n",
    "test_df['text'] = test_df['text'].apply(skip_start_and_end)\n",
    "val_df['text'] = val_df['text'].apply(skip_start_and_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c21af4-ad94-469a-820a-9e019b59005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_random_chunks(text, num_chunks=10, chunk_size=1000, overlap=False):\n",
    "#     chunk = []\n",
    "#     words = text.split(' ')\n",
    "\n",
    "#     if num_chunks * chunk_size > len(words):\n",
    "#         return text\n",
    "#     for i in range(num_chunks):\n",
    "#         new_words = []\n",
    "#         num_words = len(words)\n",
    "#         if chunk_size > num_words:\n",
    "#             chunk = chunk + words\n",
    "#             words = []\n",
    "#             return ' '.join(chunk)\n",
    "#         start = random.randint(0, num_words)\n",
    "#         chunk = [*chunk,  *words[start:start+chunk_size]]\n",
    "#         #print(chunk)\n",
    "#         if start == 0:\n",
    "#             words = words[chunk_size:]\n",
    "#         elif start == num_words - chunk_size:\n",
    "#             words = words[0:start]\n",
    "#         else:\n",
    "#             words = words[0:start] + words[start+chunk_size:]\n",
    "#     return ' '.join(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd449f-17e8-4077-804b-0efa69e0b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['chunks'] = train_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 2000, overlap=False))\n",
    "# test_df['chunks'] = test_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 2000, overlap=False))\n",
    "# val_df['chunks'] = val_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 2000, overlap=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1805ac5-f707-4725-b80d-7640e5f1682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_col = 'text'\n",
    "tokenized_col = 'tokenized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc1fda-bcca-4e40-95e8-d3c59a7318cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.contrib.concurrent import process_map\n",
    "start = time.time()\n",
    "tokenized = process_map(word_tokenize, train_df[x_col], max_workers=11, chunksize=5)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "train_df[tokenized_col] = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770c49d-a510-447e-9609-6b7fa7554b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "tokenized = process_map(word_tokenize, test_df[x_col], max_workers=11, chunksize=5)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "test_df[tokenized_col] = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2536b-f7b1-4891-9669-dc458f095552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "tokenized = process_map(word_tokenize, val_df[x_col], max_workers=11, chunksize=5)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "val_df[tokenized_col] = tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549cac0-a21a-46ef-96bf-78c02d7157e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the tokenized full text, so you don't have to run this again later\n",
    "# if you so desire\n",
    "\n",
    "outfile=os.path.join(gca_path, 'tokenized', 'train_df_full_text_tokenized.pkl')\n",
    "train_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'tokenized', 'test_df_full_text_tokenized.pkl')\n",
    "test_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'tokenized', 'val_df_full_text_tokenized.pkl')\n",
    "val_df.to_pickle(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a075e1-58e5-4b88-bf88-ffdac9087142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58f2617a-7ceb-4c30-a356-9cb723fcb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab290ac-4ee4-4364-aeee-317508bd59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_that_thang(tokenized_text):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing PorterStemmer()\n",
    "    ps = PorterStemmer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word in tokenized_text:\n",
    "        if word not in stopwords.words('english'):\n",
    "            word_final = ps.stem(word)\n",
    "            final_words.append(word_final)\n",
    "\n",
    "    return str(final_words)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb5fba-2b33-40e7-9157-7c53e47a4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_that_thang(tokenized_text):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(tokenized_text):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_final = word_lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            final_words.append(word_final)\n",
    "    return str(final_words)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb793f93-d21b-463e-925b-a1a72ef854e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78740caa-4da0-48e0-ad07-9a5077cd68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2134a9-1b81-4f5c-abba-f3c87d497334",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be7b25-5a08-4cc6-92cc-c6b93fb3cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[val_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478cae6-258e-46fd-b5ab-8848b078a1b1",
   "metadata": {},
   "source": [
    "## Lemmatize the text and save out the DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a469e97-3c60-45ea-a691-a3c3a859fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "lemmatized = process_map(lemmatize_that_thang, train_df['tokenized'], max_workers=10, chunksize=5)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "train_df['lemmatized'] = lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef55e45-3a25-43f2-9af0-960296f6737e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d32370-1fe4-44e8-9ced-f3b3bbbc2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "lemmatized = process_map(lemmatize_that_thang, val_df['tokenized'], max_workers=10, chunksize=5)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "val_df['lemmatized'] = lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1df15-e921-4e96-94e9-f8a2265407a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "lemmatized = process_map(lemmatize_that_thang, test_df['tokenized'], max_workers=10, chunksize=5)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "test_df['lemmatized'] = lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23aad2-cb3f-4631-b50c-e5d8fef3b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'train_df_full_text_lemmatized.pkl')\n",
    "train_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'val_df_full_text_lemmatized.pkl')\n",
    "val_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'test_df_full_text_lemmatized.pkl')\n",
    "test_df.to_pickle(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4c616-3c04-4ce4-b3ea-bf9ea4944067",
   "metadata": {},
   "source": [
    "# Read in the lemmatized texts, if you have already saved them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "043db7ed-ef03-490b-98cf-d1ba0a212fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pkl = os.path.join(gca_path, 'nikita_dataset', 'train_df_full_text_lemmatized.pkl')\n",
    "val_pkl = os.path.join(gca_path, 'nikita_dataset', 'val_df_full_text_lemmatized.pkl')\n",
    "test_pkl = os.path.join(gca_path, 'nikita_dataset', 'test_df_full_text_lemmatized.pkl')\n",
    "\n",
    "train_df = pd.read_pickle(train_pkl)\n",
    "val_df = pd.read_pickle(val_pkl)\n",
    "test_df = pd.read_pickle(test_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b2505-7e3b-473b-b0f8-dabad1abb190",
   "metadata": {},
   "source": [
    "## Save a version of the dataframe without the text column, for memory reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd4b0804-3085-43e9-80cc-07f07432d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('text', axis=1, inplace=True)\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'train_df_full_text_lemmatized_noRawText.pkl')\n",
    "train_df.to_pickle(outfile)\n",
    "\n",
    "val_df.drop('text', axis=1, inplace=True)\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'val_df_full_text_lemmatized_noRawText.pkl')\n",
    "val_df.to_pickle(outfile)\n",
    "\n",
    "test_df.drop('text', axis=1, inplace=True)\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'test_df_full_text_lemmatized_noRawText.pkl')\n",
    "test_df.to_pickle(outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e28b28-fb9a-4051-8957-a819cd62581e",
   "metadata": {},
   "source": [
    "## Drop both the tokenization and text columns, for memory reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db751629-f988-4912-be0c-29ca1e6b3b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['tokenized', 'text'], axis=1, inplace=True)\n",
    "val_df.drop(['tokenized', 'text'], axis=1, inplace=True)\n",
    "test_df.drop(['tokenized', 'text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002e603-7fe9-4d93-b646-7c40bf1da966",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'test_df_full_text_lemmatizedOnly.pkl')\n",
    "train_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'val_df_full_text_lemmatizedOnly.pkl')\n",
    "val_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gca_path, 'nikita_dataset', 'test_df_full_text_lemmatizedOnly.pkl')\n",
    "test_df.to_pickle(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e30cd4-acc3-4cf5-b66c-21abadbb0dd7",
   "metadata": {},
   "source": [
    "# Lemmatization, English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eeb0f61a-56cb-4aeb-8d7c-2b630cb8984e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f51ccc12-fa49-4a4d-9625-651580ad66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_col = 'tokenized'\n",
    "x_col='lemmatized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21a12718-8895-461c-8df8-268fdf3db00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Train_Y = train_df[x_col], train_df['author']\n",
    "Test_X, Test_Y = test_df[x_col], test_df['author']\n",
    "Val_X, Val_Y = val_df[x_col], val_df['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1181cc9-c4da-4287-bcc1-a055c949db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y_e = Encoder.fit_transform(Train_Y)\n",
    "Test_Y_e = Encoder.fit_transform(Test_Y)\n",
    "Val_Y_e = Encoder.fit_transform(Val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f352ea69-0a7b-44d3-8453-e2614872fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(\n",
    "    stop_words='english', # Removes a lot of common english words like it, and, that, is etc. Uses predifined scikit list of common english words.\n",
    "    sublinear_tf=True, # Uses logarithmic word frequency weighting, reducing the weight of extremely frequent terms & helps prevent domination by larger text files\n",
    "    max_features=10000, # Consideration for both overfitting and computational requirements.\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "Tfidf_vect.fit_transform(train_df[x_col])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "Val_X_Tfidf = Tfidf_vect.transform(Val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f5467-8618-4b47-a5e1-c676878f9dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0dda7617-0a96-47f0-b648-4dfab9dded78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, res_file, model_description, preproc_desc):\n",
    "    # Train and predict\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Training and predicting took {end-start} seconds = {(end-start)/60} minutes')\n",
    "\n",
    "    results={}\n",
    "    for label, y_truth, y_pred in [('train', y_train, y_train_pred), \n",
    "                            ('validation', y_val, y_val_pred),\n",
    "                            ('test', y_test, y_test_pred)]:\n",
    "        # Metrics (set zero_division=0 to silence warnings)\n",
    "        acc = accuracy_score(y_truth, y_pred)\n",
    "        f1 = f1_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "        precision = precision_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        result_dict = {'accuracy': acc,\n",
    "                       'precision': precision,\n",
    "                       'recall' : recall,\n",
    "                       'f1' : f1}\n",
    "        results[label] = result_dict\n",
    "        \n",
    "        \n",
    "    # Print performance\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f'Description: {model_description}')\n",
    "    print(f'Pre-processing: {preproc_desc}')\n",
    "    label_str=f'|{'':<15} ||  {'Accuracy':>15} | {'Precision':>15} | {'Recall':>15} | {'F1-Score':>15} |'\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    print(label_str)\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    for result_label, sub_res_dict in results.items():\n",
    "        output_str = f'|{result_label:<15} || '\n",
    "        \n",
    "        for key, val in sub_res_dict.items():\n",
    "            output_str += f' {val:15.4f} |'\n",
    "        print(output_str)\n",
    "\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    new_res_df = results_to_df(model.__class__.__name__, model_description, preproc_desc, results)\n",
    "    \n",
    "    if os.path.exists(res_file):\n",
    "        old_res_df = pd.read_csv(res_file)\n",
    "        old_res_df.set_index(['model_type', 'description', 'preprocessing description', 'metric'], inplace=True)\n",
    "    \n",
    "        res_df = pd.concat([old_res_df, new_res_df])\n",
    "        res_df.to_csv(res_file)\n",
    "    else:\n",
    "        new_res_df.to_csv(res_file)\n",
    "        \n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3d6e45d8-7c1b-4e40-9678-174f11c5ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(model_type, model_desc, preproc_desc, result_dict):\n",
    "    res_df = pd.DataFrame.from_dict(result_dict)\n",
    "    res_df['model_type'] = model_type\n",
    "    res_df['description'] = model_desc\n",
    "    res_df['preprocessing description'] = preproc_desc\n",
    "    res_df.reset_index(inplace=True)\n",
    "    res_df.rename({'index':'metric'}, axis=1, inplace=True)\n",
    "    res_df.set_index(['model_type', 'description', 'preprocessing description', 'metric'], inplace=True)\n",
    "\n",
    "    return res_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7d80c46e-2a80-4181-b4a5-a850dfed8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_those_models(Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, preprocessing_description):\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.SVC()\n",
    "    model_desc = 'default_settings'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    model_desc = 'C=1, kernel Linear, deg 3, gamma auto'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # No Effect\n",
    "    # # Classifier - Algorithm - SVM\n",
    "    # model = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='scale')\n",
    "    # model_desc='C=1, kernel Linear, deg 3 gamma scale'\n",
    "    # model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "    #                                     outfile, model_desc, preprocessing_description)\n",
    "\n",
    "\n",
    "    # Usually poor results\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # model = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto')\n",
    "    # model_desc='C=1, kernel rbf, deg 3 gamma auto'\n",
    "    # model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "    #                                     outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='scale')\n",
    "    model_desc='C=1, kernel rbf, deg 3 gamma scale'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='poly', degree=4, gamma='scale')\n",
    "    model_desc='C=1, kernel poly, deg 4 gamma scale'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='poly', degree=2, gamma='scale')\n",
    "    model_desc='C=1, kernel poly, deg 2 gamma scale'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='scale')\n",
    "    model_desc='C=1, kernel poly, deg 3 gamma scale'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # usually poor results\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # model = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto')\n",
    "    # model_desc='C=1, kernel poly, deg 3 gamma auto'\n",
    "    # model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "    #                                     outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    model = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='scale')\n",
    "    model_desc='C=1, kernel sigmoid, deg 3 gamma scale'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Usually poor results\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    # model = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto')\n",
    "    # model_desc='C=1, kernel sigmoid, deg 3 gamma auto'\n",
    "    # model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "    #                                     outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC()\n",
    "    model_desc='default'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=10)\n",
    "    model_desc='C=10'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=10, max_iter=5000)\n",
    "    model_desc='C=10, max_it 5000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=10, max_iter=50000)\n",
    "    model_desc='C=10, max_it 50000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=20, max_iter=5000)\n",
    "    model_desc='C=20, max_it 5000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=20, max_iter=50000)\n",
    "    model_desc='C=20, max_it 50000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=10, multi_class='crammer_singer', max_iter=5000)\n",
    "    model_desc='C=10, crammer-singer, max_it 5000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=10, multi_class='crammer_singer', max_iter=50000)\n",
    "    model_desc='C=10, crammer-singer, max_it 5000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=20, multi_class='crammer_singer', max_iter=5000)\n",
    "    model_desc='C=20, crammer-singer, max_it 5000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.LinearSVC(C=20, multi_class='crammer_singer', max_iter=50000)\n",
    "    model_desc='C=20, crammer-singer, max_it 50000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    model = svm.NuSVC()\n",
    "    model_desc='default'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    # fit the training dataset on the NB classifier\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model_desc='default'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    model = neural_network.MLPClassifier()\n",
    "    model_desc='default'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    model = neural_network.MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000)\n",
    "    model_desc='hidden_layer_size 200, max it 1000'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n",
    "    \n",
    "    model = neural_network.MLPClassifier(activation='logistic', max_iter=500)\n",
    "    model_desc='logistic act, 500 iter'\n",
    "    model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                        outfile, model_desc, preprocessing_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1458b-0aa2-41a5-b751-9053fd73429c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c19c373b-d86d-4ef3-a6ce-f9b7ba2733b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lemmatized'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462af034-8085-4555-96a6-be409bd227a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9db37e-04d7-4e14-af73-9661e203e795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccb7ad-078f-45d1-be62-e02bcfd6862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(\n",
    "    stop_words='english', # Removes a lot of common english words like it, and, that, is etc. Uses predifined scikit list of common english words.\n",
    "    sublinear_tf=True, # Uses logarithmic word frequency weighting, reducing the weight of extremely frequent terms & helps prevent domination by larger text files\n",
    "    max_features=10000, # Consideration for both overfitting and computational requirements.\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "Tfidf_vect.fit_transform(train_df[x_col])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "Val_X_Tfidf = Tfidf_vect.transform(Val_X)\n",
    "\n",
    "outfile='/home/dean/Documents/gitRepos/gutenberg_corpus_analysis/SVM/results_full_text_1.csv'\n",
    "preprocessing_description='tf-idf sublinear_tf true, max feat 10000, ngram (1,2)'\n",
    "train_those_models(Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, preprocessing_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52ca9f-e29e-486a-b5f0-2dbe3668a862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2b1ae-d413-403f-84bf-15c28e93319e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e96014-b091-4f44-a208-fa81b4b04228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "110d6596-d310-416d-8a95-46438534e39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 0.4978158473968506 seconds, which is 0.00829693078994751 minutes\n",
      "\n",
      "Naive Bayes Accuracy Score : Training Daata->  95.26041666666667\n",
      "Naive Bayes Accuracy Score : Test Data->  90.0\n",
      "Naive Bayes Accuracy Score : Validation Data->  87.91666666666667\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "\n",
    "predictions_NB_trainData = Naive.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_NB_val = Naive.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score : Training Daata-> \",accuracy_score(predictions_NB_trainData, Train_Y)*100)\n",
    "print(\"Naive Bayes Accuracy Score : Test Data-> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
    "print(\"Naive Bayes Accuracy Score : Validation Data-> \",accuracy_score(predictions_NB_val, Val_Y)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffde9ade-c2b1-45dd-93d3-973aaf0786ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 222.05279755592346 seconds, which is 3.700879959265391 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  99.73958333333334\n",
      "SVM Accuracy Score: Test Data ->  96.66666666666667\n",
      "SVM Accuracy Score: Validation Data ->  97.5\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f672368-69c2-41ef-be47-4cc9c2822096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 215.77153515815735 seconds, which is 3.596192252635956 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  99.73958333333334\n",
      "SVM Accuracy Score: Test Data ->  96.66666666666667\n",
      "SVM Accuracy Score: Validation Data ->  97.5\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "#Scale had no effect on linear kernel\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='scale')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc6d83-65ba-4952-8188-81303000d3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "953a5aef-343f-46b5-a740-44759a06ea95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 0.4019603729248047 seconds, which is 0.0066993395487467446 minutes\n",
      "\n",
      "Naive Bayes Accuracy Score : Training Daata->  95.26041666666667\n",
      "Naive Bayes Accuracy Score : Test Data->  90.0\n",
      "Naive Bayes Accuracy Score : Validation Data->  87.91666666666667\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "\n",
    "predictions_NB_trainData = Naive.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_NB_val = Naive.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score : Training Daata-> \",accuracy_score(predictions_NB_trainData, Train_Y)*100)\n",
    "print(\"Naive Bayes Accuracy Score : Test Data-> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
    "print(\"Naive Bayes Accuracy Score : Validation Data-> \",accuracy_score(predictions_NB_val, Val_Y)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe392162-48fe-4b74-8fa6-b6693b2cfe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 214.96361804008484 seconds, which is 3.5827269673347475 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  99.73958333333334\n",
      "SVM Accuracy Score: Test Data ->  96.66666666666667\n",
      "SVM Accuracy Score: Validation Data ->  97.5\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60fb7b44-c116-4687-89da-f007070ef0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 214.40052270889282 seconds, which is 3.573342045148214 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  99.73958333333334\n",
      "SVM Accuracy Score: Test Data ->  96.66666666666667\n",
      "SVM Accuracy Score: Validation Data ->  97.5\n"
     ]
    }
   ],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# #Scale had no effect on linear kernel\n",
    "\n",
    "# # Classifier - Algorithm - SVM\n",
    "# # fit the training dataset on the classifier\n",
    "# SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='scale')\n",
    "# SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "\n",
    "# predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "# predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "# predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "# end = time.time()\n",
    "# print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "# print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "# print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "# print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c37d19-7b1c-4205-bcc2-cd4d07251a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2edbb7d-3d4f-4327-bf46-7660536c41f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 257.4239556789398 seconds, which is 4.290399261315664 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  95.46875\n",
      "SVM Accuracy Score: Test Data ->  90.83333333333333\n",
      "SVM Accuracy Score: Validation Data ->  90.83333333333333\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "268a7302-33c2-48a1-af19-2760273926d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 230.84641885757446 seconds, which is 3.847440314292908 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  100.0\n",
      "SVM Accuracy Score: Test Data ->  92.08333333333333\n",
      "SVM Accuracy Score: Validation Data ->  95.83333333333334\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='scale')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0bf27d-1649-467f-9083-35b36ec10bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44912de2-f296-4b70-8a1f-77bbbcc0b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 220.33040642738342 seconds, which is 3.6721734404563904 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  100.0\n",
      "SVM Accuracy Score: Test Data ->  87.08333333333333\n",
      "SVM Accuracy Score: Validation Data ->  88.33333333333333\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='poly', degree=4, gamma='scale')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa4e5439-343e-481f-8997-a91872458e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 217.2997796535492 seconds, which is 3.62166299422582 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  100.0\n",
      "SVM Accuracy Score: Test Data ->  92.91666666666667\n",
      "SVM Accuracy Score: Validation Data ->  97.5\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='poly', degree=2, gamma='scale')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4913560d-13c7-4bf0-bb8e-008a65575c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 225.69400835037231 seconds, which is 3.7615668058395384 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  100.0\n",
      "SVM Accuracy Score: Test Data ->  89.58333333333334\n",
      "SVM Accuracy Score: Validation Data ->  92.91666666666667\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='scale')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05ece5e2-c40f-43a4-ac5d-8299523a02ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 234.5914843082428 seconds, which is 3.9098580718040465 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  94.79166666666666\n",
      "SVM Accuracy Score: Test Data ->  81.25\n",
      "SVM Accuracy Score: Validation Data ->  78.33333333333333\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2dccb-5c45-4741-86c8-4ff20130dab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45a1c577-22b3-40c5-83f4-c94f150a1b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 203.7535810470581 seconds, which is 3.3958930174509683 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  99.375\n",
      "SVM Accuracy Score: Test Data ->  95.83333333333334\n",
      "SVM Accuracy Score: Validation Data ->  97.08333333333333\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='scale')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd85c322-bfce-42dc-b2b6-fb407092c69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 240.62785506248474 seconds, which is 4.010464251041412 minutes\n",
      "\n",
      "SVM Accuracy Score: Training Data ->  95.52083333333333\n",
      "SVM Accuracy Score: Test Data ->  90.83333333333333\n",
      "SVM Accuracy Score: Validation Data ->  90.83333333333333\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37e1ae-f1ed-4c8f-85b8-6a70e6752907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4281772a-c3cb-4060-a254-557a51939a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 11.193020582199097 seconds, which is 0.1865503430366516 minutes\n",
      "\n",
      "LSVC Accuracy Score: Training Data ->  100.0\n",
      "LSVC Accuracy Score: Test Data ->  97.91666666666666\n",
      "LSVC Accuracy Score: Validation Data ->  99.16666666666667\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "LSVC = svm.LinearSVC()\n",
    "LSVC.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_LSVC = LSVC.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_LSVC_train = LSVC.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_LSVC_val = LSVC.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"LSVC Accuracy Score: Training Data -> \",accuracy_score(predictions_LSVC_train, Train_Y)*100)\n",
    "print(\"LSVC Accuracy Score: Test Data -> \",accuracy_score(predictions_LSVC, Test_Y)*100)\n",
    "print(\"LSVC Accuracy Score: Validation Data -> \",accuracy_score(predictions_LSVC_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8d749cf-28fd-4185-bf38-11984e05b431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 20.30560326576233 seconds, which is 0.33842672109603883 minutes\n",
      "\n",
      "LSVC Accuracy Score: Training Data ->  100.0\n",
      "LSVC Accuracy Score: Test Data ->  98.75\n",
      "LSVC Accuracy Score: Validation Data ->  99.16666666666667\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "LSVC = svm.LinearSVC(C=10)\n",
    "LSVC.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_LSVC = LSVC.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_LSVC_train = LSVC.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_LSVC_val = LSVC.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "\n",
    "print(\"LSVC Accuracy Score: Training Data -> \",accuracy_score(predictions_LSVC_train, Train_Y)*100)\n",
    "print(\"LSVC Accuracy Score: Test Data -> \",accuracy_score(predictions_LSVC, Test_Y)*100)\n",
    "print(\"LSVC Accuracy Score: Validation Data -> \",accuracy_score(predictions_LSVC_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d71764f-52e1-425d-994a-bfc262d13c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dean/.local/lib/python3.13/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 253.25499844551086 seconds, which is 4.220916640758515 minutes\n",
      "LSVC Accuracy Score: Training Data ->  100.0\n",
      "LSVC Accuracy Score: Test Data ->  98.33333333333333\n",
      "LSVC Accuracy Score: Validation Data ->  99.16666666666667\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "LSVC = svm.LinearSVC(C=10, multi_class='crammer_singer', max_iter=5000)\n",
    "LSVC.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_LSVC = LSVC.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_LSVC_train = LSVC.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_LSVC_val = LSVC.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes')\n",
    "print(\"LSVC Accuracy Score: Training Data -> \",accuracy_score(predictions_LSVC_train, Train_Y)*100)\n",
    "print(\"LSVC Accuracy Score: Test Data -> \",accuracy_score(predictions_LSVC, Test_Y)*100)\n",
    "print(\"LSVC Accuracy Score: Validation Data -> \",accuracy_score(predictions_LSVC_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c8d5943-11e6-4dce-a263-a481b6fd3a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Classifier - Algorithm - SVM\n",
    "# # fit the training dataset on the classifier\n",
    "# LSVC = svm.LinearSVC(C=10, multi_class='crammer_singer', max_iter=50000)\n",
    "# LSVC.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "# predictions_LSVC = LSVC.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "# predictions_LSVC_train = LSVC.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "# predictions_LSVC_val = LSVC.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "\n",
    "# print(\"LSVC Accuracy Score: Training Data -> \",accuracy_score(predictions_LSVC_train, Train_Y)*100)\n",
    "# print(\"LSVC Accuracy Score: Test Data -> \",accuracy_score(predictions_LSVC, Test_Y)*100)\n",
    "# print(\"LSVC Accuracy Score: Validation Data -> \",accuracy_score(predictions_LSVC_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "701c8585-85c8-46e3-9465-63a034021a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 252.76811480522156 seconds, which is 4.21280191342036 minutes\n",
      "NuSVC Accuracy Score: Training Data ->  100.0\n",
      "NuSVC Accuracy Score: Test Data ->  92.08333333333333\n",
      "NuSVC Accuracy Score: Validation Data ->  96.66666666666667\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "NuSVC_model = svm.NuSVC()\n",
    "NuSVC_model.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_NuSVC = NuSVC_model.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_NuSVC_train = NuSVC_model.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_NuSVC_val = NuSVC_model.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes')\n",
    "print(\"NuSVC Accuracy Score: Training Data -> \",accuracy_score(predictions_NuSVC_train, Train_Y)*100)\n",
    "print(\"NuSVC Accuracy Score: Test Data -> \",accuracy_score(predictions_NuSVC, Test_Y)*100)\n",
    "print(\"NuSVC Accuracy Score: Validation Data -> \",accuracy_score(predictions_NuSVC_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9274d2d-2f12-4655-b7bb-aa615ee13161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff83ca-e986-495b-8f88-2f8b434f6b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6b47ee4-bd61-457e-892f-01e1709b6525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 112.97194242477417 seconds, which is 1.8828657070795696 minutes\n",
      "MLP Accuracy Score: Training Data ->  100.0\n",
      "MLP Accuracy Score: Test Data ->  96.66666666666667\n",
      "MLP Accuracy Score: Validation Data ->  96.66666666666667\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "MLP = neural_network.MLPClassifier()\n",
    "MLP.fit(Train_X_Tfidf,Train_Y)\n",
    "#MLP.predict_proba(Val_X_Tfidf)\n",
    "\n",
    "predictions_MLP = MLP.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_MLP_train = MLP.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_MLP_val = MLP.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes')\n",
    "print(\"MLP Accuracy Score: Training Data -> \",accuracy_score(predictions_MLP_train, Train_Y)*100)\n",
    "print(\"MLP Accuracy Score: Test Data -> \",accuracy_score(predictions_MLP, Test_Y)*100)\n",
    "print(\"MLP Accuracy Score: Validation Data -> \",accuracy_score(predictions_MLP_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4df0b4-89a7-4206-b444-6486c3a83336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f53fe8d-cb20-44ba-aae3-83eebd195101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 235.98434591293335 seconds, which is 3.9330724318822226 minutes \n",
      "\n",
      "MLP Accuracy Score: Training Data ->  100.0\n",
      "MLP Accuracy Score: Test Data ->  96.66666666666667\n",
      "MLP Accuracy Score: Validation Data ->  97.08333333333333\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "MLP = neural_network.MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000)\n",
    "MLP.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "predictions_MLP = MLP.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_MLP_train = MLP.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_MLP_val = MLP.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes \\n')\n",
    "print(\"MLP Accuracy Score: Training Data -> \",accuracy_score(predictions_MLP_train, Train_Y)*100)\n",
    "print(\"MLP Accuracy Score: Test Data -> \",accuracy_score(predictions_MLP, Test_Y)*100)\n",
    "print(\"MLP Accuracy Score: Validation Data -> \",accuracy_score(predictions_MLP_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f899393-af7d-4f6c-8554-28c344cf43bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 206.57972955703735 seconds, which is 3.4429954926172894 minutes\n",
      "\n",
      "MLP Accuracy Score: Training Data ->  100.0\n",
      "MLP Accuracy Score: Test Data ->  96.66666666666667\n",
      "MLP Accuracy Score: Validation Data ->  97.5\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "MLP = neural_network.MLPClassifier(activation='logistic', max_iter=500)\n",
    "MLP.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "predictions_MLP = MLP.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_MLP_train = MLP.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_MLP_val = MLP.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "end = time.time()\n",
    "print(f'This took {end-start} seconds, which is {(end-start)/60} minutes\\n')\n",
    "\n",
    "print(\"MLP Accuracy Score: Training Data -> \",accuracy_score(predictions_MLP_train, Train_Y)*100)\n",
    "print(\"MLP Accuracy Score: Test Data -> \",accuracy_score(predictions_MLP, Test_Y)*100)\n",
    "print(\"MLP Accuracy Score: Validation Data -> \",accuracy_score(predictions_MLP_val, Val_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b817d2f-0bec-4e54-9169-713484610855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d254502-045d-4cf8-a079-5865f76c4d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4b766-1388-43bb-9340-7d587572502a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a5939-e41e-4a52-8f0e-74340deefb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300e841-6110-43ab-9343-94f32b085452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e5df4-75b0-4934-8834-fcadf39f03c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school-env",
   "language": "python",
   "name": "school-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
