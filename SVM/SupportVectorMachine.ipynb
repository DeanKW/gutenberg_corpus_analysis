{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32fbcf73-c7e7-4bf8-a071-b2ee482d6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import sys\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96fa074-d4b2-4e83-ba36-0a5298ba51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "\n",
    "gutenberg_corpus_analysis_repo = os.path.join(repos_path, 'gutenberg_corpus_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d000760-41df-41e0-8b7e-88e35178d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_repo_path = os.path.join(repos_path, 'gutenberg')\n",
    "gutenberg_analysis_repo = os.path.join(repos_path, 'gutenberg-analysis')\n",
    "\n",
    "src_dir = os.path.join(gutenberg_analysis_repo,'src')\n",
    "sys.path.append(src_dir)\n",
    "from data_io import get_book\n",
    "\n",
    "\n",
    "gutenberg_src_dir = os.path.join(gutenberg_repo_path,'src')\n",
    "sys.path.append(gutenberg_src_dir)\n",
    "\n",
    "from metaquery import meta_query\n",
    "\n",
    "sys.path.append(gutenberg_corpus_analysis_repo)\n",
    "import misc_utils.dataset_filtering as dataset_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb9aa694-08ee-4f45-92de-2721335af46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fold=os.path.join(gutenberg_repo_path, 'data', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3655ce-665a-4581-a96a-a7d629b0d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='nikita_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e80954b-c2c0-401d-8271-d0a96214f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = os.path.join(gutenberg_corpus_analysis_repo, dataset, 'final_train.csv')\n",
    "test_csv = os.path.join(gutenberg_corpus_analysis_repo, dataset, 'final_test.csv')\n",
    "val_csv = os.path.join(gutenberg_corpus_analysis_repo, dataset, 'final_val.csv')\n",
    "\n",
    "pg_catalog_filepath=os.path.join(gutenberg_repo_path, 'metadata', 'pg_catalog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebe32a1-6966-478a-94d7-b8175e422c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be able to reproduce results (won't work on chunking)\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1eb0c18-5463-45bd-b273-4436ae623067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>PG12810</td>\n",
       "      <td>Uncle Sam's Boys with Pershing's Troops: Or, D...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>78</td>\n",
       "      <td>{'World War, 1914-1918 -- Juvenile fiction', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>PG12819</td>\n",
       "      <td>Dick Prescott's Second Year at West Point: Or,...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>94</td>\n",
       "      <td>{'United States Military Academy -- Juvenile f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25920</th>\n",
       "      <td>PG40605</td>\n",
       "      <td>The Motor Boat Club at Nantucket; or, The Myst...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>189</td>\n",
       "      <td>{'Motorboats -- Juvenile fiction', 'Nantucket ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55435</th>\n",
       "      <td>PG8153</td>\n",
       "      <td>The Young Engineers in Arizona; or, Laying Tra...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>190</td>\n",
       "      <td>{'Civil engineers -- Fiction', 'Arizona -- Fic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32899</th>\n",
       "      <td>PG48863</td>\n",
       "      <td>The Motor Boat Club off Long Island; or, A Dar...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>85</td>\n",
       "      <td>{'Motorboats -- Juvenile fiction', 'Long Islan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "2439   PG12810  Uncle Sam's Boys with Pershing's Troops: Or, D...   \n",
       "2446   PG12819  Dick Prescott's Second Year at West Point: Or,...   \n",
       "25920  PG40605  The Motor Boat Club at Nantucket; or, The Myst...   \n",
       "55435   PG8153  The Young Engineers in Arizona; or, Laying Tra...   \n",
       "32899  PG48863  The Motor Boat Club off Long Island; or, A Dar...   \n",
       "\n",
       "                                   author  authoryearofbirth  \\\n",
       "2439   Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "2446   Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "25920  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "55435  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "32899  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "\n",
       "       authoryearofdeath language  downloads  \\\n",
       "2439              1922.0   ['en']         78   \n",
       "2446              1922.0   ['en']         94   \n",
       "25920             1922.0   ['en']        189   \n",
       "55435             1922.0   ['en']        190   \n",
       "32899             1922.0   ['en']         85   \n",
       "\n",
       "                                                subjects  \n",
       "2439   {'World War, 1914-1918 -- Juvenile fiction', '...  \n",
       "2446   {'United States Military Academy -- Juvenile f...  \n",
       "25920  {'Motorboats -- Juvenile fiction', 'Nantucket ...  \n",
       "55435  {'Civil engineers -- Fiction', 'Arizona -- Fic...  \n",
       "32899  {'Motorboats -- Juvenile fiction', 'Long Islan...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_csv, index_col='Unnamed: 0')\n",
    "test_df = pd.read_csv(test_csv, index_col='Unnamed: 0')\n",
    "val_df = pd.read_csv(val_csv, index_col='Unnamed: 0')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57318e71-dc3c-4978-89a7-85387744bc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df['author'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "064962ec-eb85-40b9-bf7b-492665c54a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = train_df['subjects'].replace('set()',np.nan)\n",
    "subj_docs = []\n",
    "for h in subj:\n",
    "    try:\n",
    "        h = h.strip(\"{}\")[1:-1]\n",
    "    except AttributeError:\n",
    "        subj_docs.append(h)\n",
    "        continue\n",
    "    h = h.replace(' -- ', '-')\n",
    "    h = h.replace(\"', '\",\"_\")\n",
    "    h = h.split('_')\n",
    "    h = [item.replace(' ','').replace(',', ' ') for item in h]\n",
    "    h = ' '.join(h)\n",
    "    subj_docs.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f13d40b-fb10-4122-9503-3fdd0d49d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['subj_str']=subj_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae85678-cbde-4f72-aa04-aa18687c7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df['subject_str'] = train_df['subjects'].apply(lambda x: split_subjects(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bab03ba0-50a4-4263-b70b-1e450df7746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3da0a2f0-29d6-4991-8432-05c24d333203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 5.267264366149902 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_df['text'] = train_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "test_df['text'] = test_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "val_df['text'] = val_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time elapsed: {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31a62839-5800-480c-9448-b157e495c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_start_and_end(text):\n",
    "    text = text.split(' ')\n",
    "    text = text[50:-50]\n",
    "    return ' '.join(text)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(skip_start_and_end)\n",
    "test_df['text'] = test_df['text'].apply(skip_start_and_end)\n",
    "val_df['text'] = val_df['text'].apply(skip_start_and_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "def85af7-de98-44b6-8183-71c55a8996c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_chunks(text, num_chunks=10, chunk_size=1000, overlap=False):\n",
    "    chunk = []\n",
    "    words = text.split(' ')\n",
    "\n",
    "    if num_chunks * chunk_size > len(words):\n",
    "        return text\n",
    "    for i in range(num_chunks):\n",
    "        new_words = []\n",
    "        num_words = len(words)\n",
    "        if chunk_size > num_words:\n",
    "            chunk = chunk + words\n",
    "            words = []\n",
    "            return ' '.join(chunk)\n",
    "        start = random.randint(0, num_words)\n",
    "        chunk = [*chunk,  *words[start:start+chunk_size]]\n",
    "        #print(chunk)\n",
    "        if start == 0:\n",
    "            words = words[chunk_size:]\n",
    "        elif start == num_words - chunk_size:\n",
    "            words = words[0:start]\n",
    "        else:\n",
    "            words = words[0:start] + words[start+chunk_size:]\n",
    "    return ' '.join(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d6c419-fb8d-4aaf-80a6-a1905fc19f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['chunks'] = train_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 1000, overlap=False))\n",
    "test_df['chunks'] = test_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 1000, overlap=False))\n",
    "val_df['chunks'] = val_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 1000, overlap=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d3a0452-7502-4ee8-ac66-8abab76c420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 12.728698253631592 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    train_df['tokenized'] = pool.map(word_tokenize, train_df['chunks'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8747fa30-c79a-4445-b89f-3bdad5d72ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2.776623249053955 seconds\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    val_df['tokenized'] = pool.map(word_tokenize, val_df['chunks'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee8b788e-bad3-4a35-8958-45de059976b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3.5133683681488037 seconds\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    test_df['tokenized'] = pool.map(word_tokenize, test_df['chunks'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "095b07d6-4101-47ae-bf35-78f4afd85645",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'train_df_chunks_tokenized.pkl')\n",
    "train_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'val_df_chunks_tokenized.pkl')\n",
    "val_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'test_df_chunks_tokenized.pkl')\n",
    "test_df.to_pickle(outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0c4a0-7ea1-4336-bc3f-1eb2eef98be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bb95786-4686-49f4-987c-3ba5f033ea18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>subj_str</th>\n",
       "      <th>text</th>\n",
       "      <th>chunks</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, author, authoryearofbirth, authoryearofdeath, language, downloads, subjects, subj_str, text, chunks, tokenized]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5754768-e015-4dd4-9cad-edf72f6e8c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>text</th>\n",
       "      <th>chunks</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0.1, id, title, author, authoryearofbirth, authoryearofdeath, language, downloads, subjects, text, chunks, tokenized]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c735553-42b1-4b6b-826c-08e16c6612cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>text</th>\n",
       "      <th>chunks</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0.1, id, title, author, authoryearofbirth, authoryearofdeath, language, downloads, subjects, text, chunks, tokenized]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[val_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2795859-7ed7-4d9f-aa64-d2aab1f0af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['author'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b727fad-3b10-48db-aef3-02173eb4b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03026a4a-dd24-4f93-bbdc-165219447a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(tokenized_text):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(tokenized_text):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_final = word_lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            final_words.append(word_final)\n",
    "    return str(final_words)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2113b336-d875-4362-8432-f715578960d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 265.2570126056671 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    lemmatized = pool.map(lemmatize_text, train_df['tokenized'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "train_df['lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fa10616-d82b-41ce-9025-637f9c7d3d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 34.088324546813965 seconds\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    lemmatized = pool.map(lemmatize_text, val_df['tokenized'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "val_df['lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eccdb7d2-2c4b-48ea-9452-1e60d2095f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 34.880149364471436 seconds\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    lemmatized = pool.map(lemmatize_text, test_df['tokenized'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "test_df['lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53cea7-6ccb-4cea-b25f-72f347e5cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5da91f-56e4-418a-9d91-3663f3b95b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "909eb400-a0af-4765-a776-05ee5c1c324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Train_Y = train_df['lemmatized'], train_df['author']\n",
    "Test_X, Test_Y = test_df['lemmatized'], test_df['author']\n",
    "Val_X, Val_Y = val_df['lemmatized'], val_df['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba94d71-cf70-4eac-9f5f-91a43fcb3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_X, _, Train_Y, _ = model_selection.train_test_split(train_df['lemmatized'], train_df['author'],test_size=0.3)\n",
    "# Val_X, _, Val_Y, _ = model_selection.train_test_split(val_df['lemmatized'], val_df['author'],test_size=0.3)\n",
    "# Test_X, _, Test_Y, _ = model_selection.train_test_split(test_df['lemmatized'], test_df['author'],test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed26a1-cdfd-49f3-a059-3508fc3037d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "444b2c5d-5a8a-4d1a-a186-bf9bdc8c518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y_e = Encoder.fit_transform(Train_Y)\n",
    "Test_Y_e = Encoder.fit_transform(Test_Y)\n",
    "Val_Y_e = Encoder.fit_transform(Val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d58f46-a1f0-47d1-a514-79a7335fef5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c14031f-b2b1-4d6d-91d1-0684ec6b210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(train_df['lemmatized'])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "Val_X_Tfidf = Tfidf_vect.transform(Val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6990d-f1fb-4785-ad7f-85d5d117d0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eed9a5c7-e0c8-417e-93fa-b38b2f8e1933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score: Training Data ->  99.53125\n",
      "SVM Accuracy Score: Test Data ->  82.08333333333333\n",
      "SVM Accuracy Score: Validation Data ->  81.66666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dean/anaconda3/envs/school-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dean/anaconda3/envs/school-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/dean/anaconda3/envs/school-env/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC()\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_train = SVM.predict(Train_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "predictions_SVM_val = SVM.predict(Val_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "\n",
    "\n",
    "print(\"SVM Accuracy Score: Training Data -> \",accuracy_score(predictions_SVM_train, Train_Y)*100)\n",
    "print(\"SVM Accuracy Score: Test Data -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Accuracy Score: Validation Data -> \",accuracy_score(predictions_SVM_val, Val_Y)*100)\n",
    "\n",
    "\n",
    "train_class_rpt = classification_report(Train_Y, predictions_SVM_train)\n",
    "test_class_rpt = classification_report(Val_Y, predictions_SVM_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ca6fd-5add-4411-a4fa-422eedd5d148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584e218-fa76-4207-bdcb-793424b3c7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_school_env",
   "language": "python",
   "name": "school_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
