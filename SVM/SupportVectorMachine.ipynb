{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "32fbcf73-c7e7-4bf8-a071-b2ee482d6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import sys\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm, naive_bayes, neural_network\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, make_scorer)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96fa074-d4b2-4e83-ba36-0a5298ba51fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "\n",
    "gutenberg_corpus_analysis_repo = os.path.join(repos_path, 'gutenberg_corpus_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d000760-41df-41e0-8b7e-88e35178d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_repo_path = os.path.join(repos_path, 'gutenberg')\n",
    "gutenberg_analysis_repo = os.path.join(repos_path, 'gutenberg-analysis')\n",
    "\n",
    "src_dir = os.path.join(gutenberg_analysis_repo,'src')\n",
    "sys.path.append(src_dir)\n",
    "from data_io import get_book\n",
    "\n",
    "\n",
    "gutenberg_src_dir = os.path.join(gutenberg_repo_path,'src')\n",
    "sys.path.append(gutenberg_src_dir)\n",
    "\n",
    "from metaquery import meta_query\n",
    "\n",
    "sys.path.append(gutenberg_corpus_analysis_repo)\n",
    "import misc_utils.dataset_filtering as dataset_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9197de-7222-4b9f-aab8-64bc1dc4cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be able to reproduce results (won't work on chunking)\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3655ce-665a-4581-a96a-a7d629b0d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='nikita_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e80954b-c2c0-401d-8271-d0a96214f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = os.path.join(gutenberg_corpus_analysis_repo, dataset, 'final_train.csv')\n",
    "test_csv = os.path.join(gutenberg_corpus_analysis_repo, dataset, 'final_test.csv')\n",
    "val_csv = os.path.join(gutenberg_corpus_analysis_repo, dataset, 'final_val.csv')\n",
    "\n",
    "pg_catalog_filepath=os.path.join(gutenberg_repo_path, 'metadata', 'pg_catalog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebe32a1-6966-478a-94d7-b8175e422c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1eb0c18-5463-45bd-b273-4436ae623067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>PG12810</td>\n",
       "      <td>Uncle Sam's Boys with Pershing's Troops: Or, D...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>78</td>\n",
       "      <td>{'World War, 1914-1918 -- Juvenile fiction', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>PG12819</td>\n",
       "      <td>Dick Prescott's Second Year at West Point: Or,...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>94</td>\n",
       "      <td>{'United States Military Academy -- Juvenile f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25920</th>\n",
       "      <td>PG40605</td>\n",
       "      <td>The Motor Boat Club at Nantucket; or, The Myst...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>189</td>\n",
       "      <td>{'Motorboats -- Juvenile fiction', 'Nantucket ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55435</th>\n",
       "      <td>PG8153</td>\n",
       "      <td>The Young Engineers in Arizona; or, Laying Tra...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>190</td>\n",
       "      <td>{'Civil engineers -- Fiction', 'Arizona -- Fic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32899</th>\n",
       "      <td>PG48863</td>\n",
       "      <td>The Motor Boat Club off Long Island; or, A Dar...</td>\n",
       "      <td>Hancock, H. Irving (Harrie Irving)</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>85</td>\n",
       "      <td>{'Motorboats -- Juvenile fiction', 'Long Islan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "2439   PG12810  Uncle Sam's Boys with Pershing's Troops: Or, D...   \n",
       "2446   PG12819  Dick Prescott's Second Year at West Point: Or,...   \n",
       "25920  PG40605  The Motor Boat Club at Nantucket; or, The Myst...   \n",
       "55435   PG8153  The Young Engineers in Arizona; or, Laying Tra...   \n",
       "32899  PG48863  The Motor Boat Club off Long Island; or, A Dar...   \n",
       "\n",
       "                                   author  authoryearofbirth  \\\n",
       "2439   Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "2446   Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "25920  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "55435  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "32899  Hancock, H. Irving (Harrie Irving)             1868.0   \n",
       "\n",
       "       authoryearofdeath language  downloads  \\\n",
       "2439              1922.0   ['en']         78   \n",
       "2446              1922.0   ['en']         94   \n",
       "25920             1922.0   ['en']        189   \n",
       "55435             1922.0   ['en']        190   \n",
       "32899             1922.0   ['en']         85   \n",
       "\n",
       "                                                subjects  \n",
       "2439   {'World War, 1914-1918 -- Juvenile fiction', '...  \n",
       "2446   {'United States Military Academy -- Juvenile f...  \n",
       "25920  {'Motorboats -- Juvenile fiction', 'Nantucket ...  \n",
       "55435  {'Civil engineers -- Fiction', 'Arizona -- Fic...  \n",
       "32899  {'Motorboats -- Juvenile fiction', 'Long Islan...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_csv, index_col='Unnamed: 0')\n",
    "test_df = pd.read_csv(test_csv, index_col='Unnamed: 0')\n",
    "val_df = pd.read_csv(val_csv, index_col='Unnamed: 0')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57318e71-dc3c-4978-89a7-85387744bc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df['author'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "064962ec-eb85-40b9-bf7b-492665c54a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = train_df['subjects'].replace('set()',np.nan)\n",
    "subj_docs = []\n",
    "for h in subj:\n",
    "    try:\n",
    "        h = h.strip(\"{}\")[1:-1]\n",
    "    except AttributeError:\n",
    "        subj_docs.append(h)\n",
    "        continue\n",
    "    h = h.replace(' -- ', '-')\n",
    "    h = h.replace(\"', '\",\"_\")\n",
    "    h = h.split('_')\n",
    "    h = [item.replace(' ','').replace(',', ' ') for item in h]\n",
    "    h = ' '.join(h)\n",
    "    subj_docs.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f13d40b-fb10-4122-9503-3fdd0d49d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['subj_str']=subj_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae85678-cbde-4f72-aa04-aa18687c7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df['subject_str'] = train_df['subjects'].apply(lambda x: split_subjects(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bab03ba0-50a4-4263-b70b-1e450df7746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3da0a2f0-29d6-4991-8432-05c24d333203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 5.267264366149902 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_df['text'] = train_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "test_df['text'] = test_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "val_df['text'] = val_df['id'].apply(lambda x: get_book(x, path_gutenberg=gutenberg_repo_path,level='text'))\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time elapsed: {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6010e4b5-6a68-4cc6-89cc-35a5bbd48c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the word, line and token counts\n",
    "def enrich_dataframe(df):\n",
    "    count_path = os.path.join(gutenberg_repo_path, 'data', 'counts')\n",
    "    text_path = os.path.join(gutenberg_repo_path, 'data', 'text')\n",
    "    token_path = os.path.join(gutenberg_repo_path, 'data', 'tokens')\n",
    "\n",
    "    df['word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_word_count(pid, count_path))\n",
    "    df['unique_word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_unique_word_count(pid, count_path))\n",
    "    df['line_count'] = df['id'].apply(lambda pid: dataset_filtering.get_line_count(pid, text_path))\n",
    "    df['token_count'] = df['id'].apply(lambda pid: dataset_filtering.get_token_count(pid, token_path))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ef0ae-0fef-4ada-9378-034d2890a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = enrich_dataframe(train_df)\n",
    "# val_df = enrich_dataframe(tval_df)\n",
    "# test_df = enrich_dataframe(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31a62839-5800-480c-9448-b157e495c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_start_and_end(text):\n",
    "    text = text.split(' ')\n",
    "    text = text[50:-50]\n",
    "    return ' '.join(text)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(skip_start_and_end)\n",
    "test_df['text'] = test_df['text'].apply(skip_start_and_end)\n",
    "val_df['text'] = val_df['text'].apply(skip_start_and_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "def85af7-de98-44b6-8183-71c55a8996c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_chunks(text, num_chunks=10, chunk_size=1000, overlap=False):\n",
    "    chunk = []\n",
    "    words = text.split(' ')\n",
    "\n",
    "    if num_chunks * chunk_size > len(words):\n",
    "        return text\n",
    "    for i in range(num_chunks):\n",
    "        new_words = []\n",
    "        num_words = len(words)\n",
    "        if chunk_size > num_words:\n",
    "            chunk = chunk + words\n",
    "            words = []\n",
    "            return ' '.join(chunk)\n",
    "        start = random.randint(0, num_words)\n",
    "        chunk = [*chunk,  *words[start:start+chunk_size]]\n",
    "        #print(chunk)\n",
    "        if start == 0:\n",
    "            words = words[chunk_size:]\n",
    "        elif start == num_words - chunk_size:\n",
    "            words = words[0:start]\n",
    "        else:\n",
    "            words = words[0:start] + words[start+chunk_size:]\n",
    "    return ' '.join(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d6c419-fb8d-4aaf-80a6-a1905fc19f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['chunks'] = train_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 1000, overlap=False))\n",
    "test_df['chunks'] = test_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 1000, overlap=False))\n",
    "val_df['chunks'] = val_df['text'].apply(lambda x: make_random_chunks(x, num_chunks=10, chunk_size = 1000, overlap=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d3a0452-7502-4ee8-ac66-8abab76c420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 12.728698253631592 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    train_df['tokenized'] = pool.map(word_tokenize, train_df['chunks'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8747fa30-c79a-4445-b89f-3bdad5d72ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2.776623249053955 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    val_df['tokenized'] = pool.map(word_tokenize, val_df['chunks'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee8b788e-bad3-4a35-8958-45de059976b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3.5133683681488037 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    test_df['tokenized'] = pool.map(word_tokenize, test_df['chunks'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "095b07d6-4101-47ae-bf35-78f4afd85645",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'train_df_chunks_tokenized.pkl')\n",
    "train_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'val_df_chunks_tokenized.pkl')\n",
    "val_df.to_pickle(outfile)\n",
    "\n",
    "outfile=os.path.join(gutenberg_corpus_analysis_repo, 'tokenized', 'test_df_chunks_tokenized.pkl')\n",
    "test_df.to_pickle(outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d0c4a0-7ea1-4336-bc3f-1eb2eef98be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bb95786-4686-49f4-987c-3ba5f033ea18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>subj_str</th>\n",
       "      <th>text</th>\n",
       "      <th>chunks</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, author, authoryearofbirth, authoryearofdeath, language, downloads, subjects, subj_str, text, chunks, tokenized]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5754768-e015-4dd4-9cad-edf72f6e8c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>text</th>\n",
       "      <th>chunks</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0.1, id, title, author, authoryearofbirth, authoryearofdeath, language, downloads, subjects, text, chunks, tokenized]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c735553-42b1-4b6b-826c-08e16c6612cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>text</th>\n",
       "      <th>chunks</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0.1, id, title, author, authoryearofbirth, authoryearofdeath, language, downloads, subjects, text, chunks, tokenized]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[val_df['tokenized'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2795859-7ed7-4d9f-aa64-d2aab1f0af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['author'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b727fad-3b10-48db-aef3-02173eb4b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03026a4a-dd24-4f93-bbdc-165219447a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(tokenized_text):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(tokenized_text):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_final = word_lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            final_words.append(word_final)\n",
    "    return str(final_words)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2113b336-d875-4362-8432-f715578960d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 265.2570126056671 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    lemmatized = pool.map(lemmatize_text, train_df['tokenized'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "train_df['lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4fa10616-d82b-41ce-9025-637f9c7d3d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 34.088324546813965 seconds\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    lemmatized = pool.map(lemmatize_text, val_df['tokenized'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "val_df['lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eccdb7d2-2c4b-48ea-9452-1e60d2095f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 34.880149364471436 seconds\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "start = time.time()\n",
    "with mp.Pool(11) as pool:\n",
    "    lemmatized = pool.map(lemmatize_text, test_df['tokenized'])\n",
    "end = time.time()\n",
    "print(f'Took {end-start} seconds')\n",
    "test_df['lemmatized'] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53cea7-6ccb-4cea-b25f-72f347e5cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5da91f-56e4-418a-9d91-3663f3b95b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "909eb400-a0af-4765-a776-05ee5c1c324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Train_Y = train_df['lemmatized'], train_df['author']\n",
    "Test_X, Test_Y = test_df['lemmatized'], test_df['author']\n",
    "Val_X, Val_Y = val_df['lemmatized'], val_df['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba94d71-cf70-4eac-9f5f-91a43fcb3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_X, _, Train_Y, _ = model_selection.train_test_split(train_df['lemmatized'], train_df['author'],test_size=0.3)\n",
    "# Val_X, _, Val_Y, _ = model_selection.train_test_split(val_df['lemmatized'], val_df['author'],test_size=0.3)\n",
    "# Test_X, _, Test_Y, _ = model_selection.train_test_split(test_df['lemmatized'], test_df['author'],test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed26a1-cdfd-49f3-a059-3508fc3037d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "444b2c5d-5a8a-4d1a-a186-bf9bdc8c518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LabelEncoder()\n",
    "# Train_Y_e = Encoder.fit_transform(Train_Y)\n",
    "# Test_Y_e = Encoder.fit_transform(Test_Y)\n",
    "# Val_Y_e = Encoder.fit_transform(Val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d58f46-a1f0-47d1-a514-79a7335fef5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c14031f-b2b1-4d6d-91d1-0684ec6b210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(train_df['lemmatized'])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "Val_X_Tfidf = Tfidf_vect.transform(Val_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fba0ed-584e-43a7-8e7f-133641e2ed3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "b1da9f0d-a9d2-4655-8871-4d5300599242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, res_file, model_description, preproc_desc):\n",
    "    # Train and predict\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Training and predicting took {end-start} seconds = {(end-start)/60} minutes')\n",
    "\n",
    "    results={}\n",
    "    for label, y_truth, y_pred in [('train', y_train, y_train_pred), \n",
    "                            ('validation', y_val, y_val_pred),\n",
    "                            ('test', y_test, y_test_pred)]:\n",
    "        # Metrics (set zero_division=0 to silence warnings)\n",
    "        acc = accuracy_score(y_truth, y_pred)\n",
    "        f1 = f1_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "        precision = precision_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_truth, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        result_dict = {'accuracy': acc,\n",
    "                       'precision': precision,\n",
    "                       'recall' : recall,\n",
    "                       'f1' : f1}\n",
    "        results[label] = result_dict\n",
    "        \n",
    "        \n",
    "    # Print performance\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    label_str=f'|{'':<15} ||  {'Accuracy':>15} | {'Precision':>15} | {'Recall':>15} | {'F1-Score':>15} |'\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    print(label_str)\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    for result_label, sub_res_dict in results.items():\n",
    "        output_str = f'|{result_label:<15} || '\n",
    "        \n",
    "        for key, val in sub_res_dict.items():\n",
    "            output_str += f' {val:15.4f} |'\n",
    "        print(output_str)\n",
    "\n",
    "    print(\"-\" * len(label_str))\n",
    "\n",
    "    new_res_df = results_to_df(model.__class__.__name__, model_description, preproc_desc, results)\n",
    "    \n",
    "    if os.path.exists(res_file):\n",
    "        old_res_df = pd.read_csv(res_file)\n",
    "        old_res_df.set_index(['model_type', 'description', 'preprocessing description', 'metric'], inplace=True)\n",
    "    \n",
    "        res_df = pd.concat([old_res_df, new_res_df])\n",
    "        res_df.to_csv(res_file)\n",
    "    else:\n",
    "        new_res_df.to_csv(res_file)\n",
    "        \n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "96722785-e12f-49a0-8086-93c54df4d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(model_type, model_desc, preproc_desc, result_dict):\n",
    "    res_df = pd.DataFrame.from_dict(result_dict)\n",
    "    res_df['model_type'] = model_type\n",
    "    res_df['description'] = model_desc\n",
    "    res_df['preprocessing description'] = preproc_desc\n",
    "    res_df.reset_index(inplace=True)\n",
    "    res_df.rename({'index':'metric'}, axis=1, inplace=True)\n",
    "    res_df.set_index(['model_type', 'description', 'preprocessing description', 'metric'], inplace=True)\n",
    "\n",
    "    return res_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de20fb-73b6-407c-9e34-d09a26b998e7",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472d1f8-19a0-417f-85b7-dbcb1cb48fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "513c3140-80ec-4623-8e30-5aa535b865e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile='/home/dean/Documents/gitRepos/gutenberg_corpus_analysis/SVM/results.csv'\n",
    "preprocessing_description = '10 chunks, 10000 long, tf-idf 5000 max feat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "eed9a5c7-e0c8-417e-93fa-b38b2f8e1933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 101.408536195755 seconds = 1.69014226992925 minutes\n",
      "Model: SVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.9953 |          0.9956 |          0.9953 |          0.9954 |\n",
      "|validation      ||           0.8167 |          0.8533 |          0.8167 |          0.8104 |\n",
      "|test            ||           0.8208 |          0.8579 |          0.8208 |          0.8164 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "model = svm.SVC()\n",
    "model_desc = 'default_settings'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)\n",
    "\n",
    "#train_class_rpt = classification_report(Train_Y, predictions_SVM_train)\n",
    "#test_class_rpt = classification_report(Val_Y, predictions_SVM_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "517ca6fd-5add-4411-a4fa-422eedd5d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 100.77923703193665 seconds = 1.6796539505322774 minutes\n",
      "Model: SVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.9833 |          0.9846 |          0.9833 |          0.9835 |\n",
      "|validation      ||           0.8542 |          0.8808 |          0.8542 |          0.8477 |\n",
      "|test            ||           0.8500 |          0.8577 |          0.8500 |          0.8390 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "model = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "model_desc = 'C=1, kernel Linear, deg 3, gamma auto'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "d584e218-fa76-4207-bdcb-793424b3c7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 106.44879674911499 seconds = 1.7741466124852499 minutes\n",
      "Model: SVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.9833 |          0.9846 |          0.9833 |          0.9835 |\n",
      "|validation      ||           0.8542 |          0.8808 |          0.8542 |          0.8477 |\n",
      "|test            ||           0.8500 |          0.8577 |          0.8500 |          0.8390 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "model = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='scale')\n",
    "model_desc='C=1, kernel Linear, deg 3 gamma scale'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "bad14a5a-472b-4777-862a-697df8129d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 112.20486640930176 seconds = 1.870081106821696 minutes\n",
      "Model: SVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.8677 |          0.8902 |          0.8677 |          0.8641 |\n",
      "|validation      ||           0.7375 |          0.7884 |          0.7375 |          0.7242 |\n",
      "|test            ||           0.6833 |          0.7462 |          0.6833 |          0.6709 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "model = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto')\n",
    "model_desc='C=1, kernel rbf, deg 3 gamma auto'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "d458203e-7b39-4071-8543-3ac024234f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 107.68566799163818 seconds = 1.7947611331939697 minutes\n",
      "Model: SVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.9953 |          0.9956 |          0.9953 |          0.9954 |\n",
      "|validation      ||           0.8167 |          0.8533 |          0.8167 |          0.8104 |\n",
      "|test            ||           0.8208 |          0.8579 |          0.8208 |          0.8164 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "model = svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='scale')\n",
    "model_desc='C=1, kernel rbf, deg 3 gamma scale'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "9a41bd47-ac75-4f5f-8769-df58d2e0d677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 110.0511474609375 seconds = 1.834185791015625 minutes\n",
      "Model: SVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.9995 |          0.9995 |          0.9995 |          0.9995 |\n",
      "|validation      ||           0.7042 |          0.7820 |          0.7042 |          0.7116 |\n",
      "|test            ||           0.7250 |          0.8052 |          0.7250 |          0.7347 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "model = svm.SVC(C=1.0, kernel='poly', degree=4, gamma='scale')\n",
    "model_desc='C=1, kernel poly, deg 4 gamma scale'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "57130000-6b49-48cd-ae7e-29ec3dc548c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 107.77263116836548 seconds = 1.796210519472758 minutes\n",
      "Model: SVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.9969 |          0.9971 |          0.9969 |          0.9969 |\n",
      "|validation      ||           0.8208 |          0.8445 |          0.8208 |          0.8094 |\n",
      "|test            ||           0.8167 |          0.8506 |          0.8167 |          0.8113 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "model = svm.SVC(C=1.0, kernel='poly', degree=2, gamma='scale')\n",
    "model_desc='C=1, kernel poly, deg 2 gamma scale'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "303050c2-a040-42cf-8ea2-3f08ffda4448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 110.00115394592285 seconds = 1.8333525657653809 minutes\n",
      "Model: SVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.9995 |          0.9995 |          0.9995 |          0.9995 |\n",
      "|validation      ||           0.7625 |          0.8155 |          0.7625 |          0.7568 |\n",
      "|test            ||           0.7542 |          0.8041 |          0.7542 |          0.7531 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "model = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='scale')\n",
    "model_desc='C=1, kernel poly, deg 3 gamma scale'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "6d23a14a-1d14-4c9f-b9c2-2f058e4cf495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 108.22327303886414 seconds = 1.8037212173144022 minutes\n",
      "Model: SVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.9359 |          0.9532 |          0.9359 |          0.9345 |\n",
      "|validation      ||           0.6417 |          0.7669 |          0.6417 |          0.6592 |\n",
      "|test            ||           0.6125 |          0.7504 |          0.6125 |          0.6306 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "model = svm.SVC(C=1.0, kernel='poly', degree=3, gamma='auto')\n",
    "model_desc='C=1, kernel poly, deg 3 gamma auto'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "fe9cbabf-b9d7-40c7-b4c6-9cb25b8d3d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 95.98268699645996 seconds = 1.5997114499409995 minutes\n",
      "Model: SVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.9656 |          0.9683 |          0.9656 |          0.9660 |\n",
      "|validation      ||           0.8500 |          0.8756 |          0.8500 |          0.8460 |\n",
      "|test            ||           0.8250 |          0.8600 |          0.8250 |          0.8186 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "model = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='scale')\n",
    "model_desc='C=1, kernel sigmoid, deg 3 gamma scale'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "69b49f58-922b-4d8c-b585-25cccb3551b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 108.43932104110718 seconds = 1.8073220173517863 minutes\n",
      "Model: SVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.8677 |          0.8902 |          0.8677 |          0.8641 |\n",
      "|validation      ||           0.7375 |          0.7884 |          0.7375 |          0.7242 |\n",
      "|test            ||           0.6833 |          0.7462 |          0.6833 |          0.6709 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "model = svm.SVC(C=1.0, kernel='sigmoid', degree=3, gamma='auto')\n",
    "model_desc='C=1, kernel sigmoid, deg 3 gamma auto'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1accc88-fe89-4de5-b3ec-c46a603f7bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "2fbff5a6-2923-4d4c-aa43-9de2e145aaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 5.753037929534912 seconds = 0.09588396549224854 minutes\n",
      "Model: LinearSVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9167 |          0.9419 |          0.9167 |          0.9134 |\n",
      "|test            ||           0.8958 |          0.8933 |          0.8958 |          0.8847 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "model = svm.LinearSVC()\n",
    "model_desc='default'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "9ac05056-abbc-4089-b2fe-7d7453f42bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 7.698090314865112 seconds = 0.12830150524775188 minutes\n",
      "Model: LinearSVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9250 |          0.9419 |          0.9250 |          0.9212 |\n",
      "|test            ||           0.9167 |          0.9267 |          0.9167 |          0.9103 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "model = svm.LinearSVC(C=10)\n",
    "model_desc='C=10'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "3038ab5f-5946-4632-a10d-4e70cc217613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 17.52768325805664 seconds = 0.292128054300944 minutes\n",
      "Model: LinearSVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9208 |          0.9427 |          0.9208 |          0.9174 |\n",
      "|test            ||           0.9125 |          0.9235 |          0.9125 |          0.9076 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dean/anaconda3/envs/school-env/lib/python3.12/site-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "model = svm.LinearSVC(C=10, multi_class='crammer_singer', max_iter=5000)\n",
    "model_desc='C=10, crammer-singer, max_it 5000'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "02bd4a07-e61c-4391-94bd-38c3bb8ac1ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 102.72420239448547 seconds = 1.7120700399080913 minutes\n",
      "Model: NuSVC\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.9948 |          0.9953 |          0.9948 |          0.9949 |\n",
      "|validation      ||           0.8375 |          0.8539 |          0.8375 |          0.8270 |\n",
      "|test            ||           0.8250 |          0.8567 |          0.8250 |          0.8197 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "model = svm.NuSVC()\n",
    "model_desc='default'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9efac50-378e-484a-87cd-977788323758",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "d8b18b7d-ccbd-4ed6-a72f-a6df8aa1fd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 0.18079328536987305 seconds = 0.0030132214228312175 minutes\n",
      "Model: MultinomialNB\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.9005 |          0.9287 |          0.9005 |          0.9030 |\n",
      "|validation      ||           0.6708 |          0.7280 |          0.6708 |          0.6462 |\n",
      "|test            ||           0.7042 |          0.7653 |          0.7042 |          0.6843 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "model = naive_bayes.MultinomialNB()\n",
    "model_desc='default'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ca023-f4ae-4651-9afc-bb70b01d3eb2",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d96d3bf-44aa-4e04-ac38-9b34d01771fc",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "16fd870f-1add-4c0b-b25a-dc7a88339dc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 51.74481916427612 seconds = 0.8624136527379354 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9125 |          0.9321 |          0.9125 |          0.9088 |\n",
      "|test            ||           0.9167 |          0.9279 |          0.9167 |          0.9135 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier()\n",
    "model_desc='default'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "40ea159e-099e-421b-8660-cb32f10614f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 87.38759231567383 seconds = 1.4564598719278972 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9208 |          0.9396 |          0.9208 |          0.9186 |\n",
      "|test            ||           0.9125 |          0.9173 |          0.9125 |          0.9062 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,), max_iter=1000)\n",
    "model_desc='hidden_layer_size 200, max it 1000'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "12f790fd-1f76-499f-9d1a-f483d72794ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 99.63221025466919 seconds = 1.6605368375778198 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9167 |          0.9344 |          0.9167 |          0.9151 |\n",
      "|test            ||           0.9042 |          0.9162 |          0.9042 |          0.8997 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(activation='logistic', max_iter=500)\n",
    "model_desc='logistic act, 500 iter'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "754ff6d0-b720-4d09-bba4-e0b5ca2ceb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 273.6840138435364 seconds = 4.5614002307256065 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9083 |          0.9356 |          0.9083 |          0.9081 |\n",
      "|test            ||           0.9250 |          0.9350 |          0.9250 |          0.9187 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,), activation='logistic', max_iter=500)\n",
    "model_desc='hidden layer size 200, logistic act, 500 iter'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "d8fa8c00-b818-475c-be3e-70d14924eb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 8.986948013305664 seconds = 0.14978246688842772 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "|validation      ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "|test            ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,), activation='logistic', max_iter=500, learning_rate='invscaling', solver='sgd')\n",
    "model_desc='hidden layer size 200, logistic act, 500 iter, sgd-invscaling'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "3ad3df5a-13d1-48f6-9700-c3b0c99f50f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 21.707059860229492 seconds = 0.36178433100382484 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0109 |          0.0005 |          0.0109 |          0.0009 |\n",
      "|validation      ||           0.0125 |          0.0005 |          0.0125 |          0.0010 |\n",
      "|test            ||           0.0083 |          0.0002 |          0.0083 |          0.0004 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,), activation='relu', max_iter=500, learning_rate='invscaling', solver='sgd')\n",
    "model_desc='hidden layer size 500, relu act, 500 iter, sgd-invscaling'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "29c74b1e-2c00-4bad-95f0-52e77d0352a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dean/anaconda3/envs/school-env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 851.5928704738617 seconds = 14.193214507897695 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.4568 |          0.7121 |          0.4568 |          0.4622 |\n",
      "|validation      ||           0.3250 |          0.3742 |          0.3250 |          0.3133 |\n",
      "|test            ||           0.3375 |          0.3766 |          0.3375 |          0.3095 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,), activation='relu', max_iter=500, learning_rate='constant', solver='sgd')\n",
    "model_desc='hidden layer size 500, relu act, 500 iter, sgd-constant'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "6eb0f85d-3eb2-4d65-ba15-b2a5ba17c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 185.4526436328888 seconds = 3.0908773938814798 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.9292 |          0.9440 |          0.9292 |          0.9265 |\n",
      "|test            ||           0.9167 |          0.9225 |          0.9167 |          0.9103 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 500, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "e684cc06-e386-4b98-b14c-d9b951cca409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 23.782085180282593 seconds = 0.3963680863380432 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0146 |          0.0004 |          0.0146 |          0.0007 |\n",
      "|validation      ||           0.0083 |          0.0002 |          0.0083 |          0.0004 |\n",
      "|test            ||           0.0083 |          0.0002 |          0.0083 |          0.0005 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,200, 100, ), activation='relu', max_iter=500, learning_rate='invscaling', solver='sgd')\n",
    "model_desc='hidden layer size 500-200-100, relu act, 500 iter, sgd-invscaling'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd3524-71f6-45c4-8e91-8a7f62d65bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,200, 100, ), activation='relu', max_iter=500, learning_rate='constant', solver='sgd')\n",
    "model_desc='hidden layer size 500-200-100, relu act, 500 iter, sgd-constant'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "e62731c6-bdd5-4990-83c8-1bcfa81f9ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 127.03732872009277 seconds = 2.1172888120015463 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8833 |          0.9142 |          0.8833 |          0.8776 |\n",
      "|test            ||           0.8375 |          0.8545 |          0.8375 |          0.8272 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(500,200, 100, ), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 500-200-100, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "a51dfd96-f761-4149-920a-29e1a9c1573a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 8.672466516494751 seconds = 0.14454110860824584 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0135 |          0.0008 |          0.0135 |          0.0013 |\n",
      "|validation      ||           0.0167 |          0.0010 |          0.0167 |          0.0017 |\n",
      "|test            ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,100, ), activation='relu', max_iter=500, learning_rate='invscaling', solver='sgd')\n",
    "model_desc='hidden layer size 200-100, relu act, 500 iter, sgd-invscaling'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "1b03d26a-3ccc-4e06-985a-c8fde3bde853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 67.76165580749512 seconds = 1.1293609301249186 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           1.0000 |          1.0000 |          1.0000 |          1.0000 |\n",
      "|validation      ||           0.8958 |          0.9117 |          0.8958 |          0.8930 |\n",
      "|test            ||           0.9000 |          0.9237 |          0.9000 |          0.8950 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,100, ), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 200-100, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "883745bc-d091-4cae-9c61-77b9c48753fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting took 8.011568307876587 seconds = 0.13352613846460978 minutes\n",
      "Model: MLPClassifier\n",
      "--------------------------------------------------------------------------------------------\n",
      "|                ||         Accuracy |       Precision |          Recall |        F1-Score |\n",
      "--------------------------------------------------------------------------------------------\n",
      "|train           ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "|validation      ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "|test            ||           0.0125 |          0.0002 |          0.0125 |          0.0003 |\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,200, ), activation='relu', max_iter=500, learning_rate='invscaling', solver='sgd')\n",
    "model_desc='hidden layer size 200-200, relu act, 500 iter, sgd-invscaling'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36d340-b0c0-4a2e-b776-23fbb6318517",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,200, ), activation='relu', max_iter=500, learning_rate='constant', solver='sgd')\n",
    "model_desc='hidden layer size 200-200, relu act, 500 iter, sgd-constant'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be55c5d-4d6c-4df8-9b03-cc215be39747",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(200,200, ), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 200-200, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eccf56-cf9e-4fc9-b9d1-6ab48d67aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(100,50, ), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 100-50, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c6686-0928-4a1d-a097-c6411abad82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(100,100, ), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 100-100, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d368166-9fee-4861-b5df-353072877fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neural_network.MLPClassifier(hidden_layer_sizes=(100,100, 100,), activation='relu', max_iter=500, solver='adam')\n",
    "model_desc='hidden layer size 100-100-100, relu act, 500 iter, adam'\n",
    "model, result_dict = evaluate_model(model, Train_X_Tfidf, Train_Y, Val_X_Tfidf, Val_Y, Test_X_Tfidf, Test_Y, \n",
    "                                    outfile, model_desc, preprocessing_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fea38d-dc9c-4de2-9fdb-54f3fcd4f875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_school_env",
   "language": "python",
   "name": "school_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
